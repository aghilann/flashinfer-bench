{
  "name": "cutile-moe",
  "definition": "moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048",
  "author": "aghilan",
  "spec": {
    "language": "python",
    "target_hardware": [
      "cuda"
    ],
    "entry_point": "hardcoded.py::run",
    "dependencies": [],
    "destination_passing_style": true
  },
  "sources": [
    {
      "path": "hardcoded.py",
      "content": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: MIT\n\n\"\"\"\nMoE Kernel for FlashInfer Competition - DeepSeek-V3 MoE Layer.\n\nVERSION: v4-hardcoded - CuTile with Hardcoded Optimal Tile Sizes:\n1. Optimized silu_and_mul kernel from TileGym\n2. Group GEMM with hardcoded optimal tile configurations (no autotuning)\n3. Vectorized token sorting\n4. Tile size selection based on workload size (small vs large)\n\nOptimal tile sizes determined by autotuning on NVIDIA B200:\n- Small workloads (tokens_per_expert < 100): TILE_M=64, TILE_N=128, TILE_K=64, occupancy=2\n- Large workloads (tokens_per_expert >= 100): TILE_M=256, TILE_N=256, TILE_K=64, num_ctas=2\n\"\"\"\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\n# Threshold for switching between small and large tile configs\n# Based on autotuning results: small config better when M < 100\nLARGE_WORKLOAD_THRESHOLD = 100\n\nimport cuda.tile as ct\nimport torch\nfrom typing import Tuple, List\nfrom cuda.tile._numeric_semantics import RoundingMode as RMd\n\nConstInt = ct.Constant[int]\nConstBool = ct.Constant[bool]\n\n\ndef next_power_of_2(n):\n    \"\"\"Return the next power of 2 >= n.\"\"\"\n    if n <= 0:\n        return 1\n    n -= 1\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    return n + 1\n\n\n# ============================================================================\n# FP8 Dequantization\n# ============================================================================\n\ndef dequantize_hidden_states(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 hidden states with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_t = scales.permute(1, 0).contiguous()\n    scales_expanded = scales_t.repeat_interleave(BLOCK, dim=1)\n    return data_f32 * scales_expanded\n\n\ndef dequantize_weights_batched(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weight matrices with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_exp = scales.repeat_interleave(BLOCK, dim=1)\n    scales_exp = scales_exp.repeat_interleave(BLOCK, dim=2)\n    return data_f32 * scales_exp\n\n\n# ============================================================================\n# Routing\n# ============================================================================\n\ndef deepseek_routing(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    routed_scaling_factor: float,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"DeepSeek-V3 no-aux-loss routing.\"\"\"\n    T, E_global = routing_logits.shape\n    \n    bias = routing_bias.to(torch.float32)\n    s = torch.sigmoid(routing_logits)\n    s_with_bias = s + bias\n    \n    group_size = E_global // N_GROUP\n    s_wb_grouped = s_with_bias.view(T, N_GROUP, group_size)\n    top2_vals, _ = torch.topk(s_wb_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)\n    \n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1, largest=True, sorted=False)\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size).reshape(T, E_global)\n    \n    scores_pruned = torch.where(score_mask > 0, s_with_bias, torch.finfo(torch.float32).min)\n    _, topk_idx = torch.topk(scores_pruned, k=TOP_K, dim=1, largest=True, sorted=False)\n    \n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights_sum = weights.sum(dim=1, keepdim=True) + 1e-20\n    weights = (weights / weights_sum) * routed_scaling_factor\n    \n    return weights, topk_idx\n\n\n# ============================================================================\n# Token Sorting (Vectorized)\n# ============================================================================\n\ndef sort_tokens_by_expert(\n    topk_idx: torch.Tensor,\n    weights: torch.Tensor,\n    E_local: int,\n    local_expert_offset: int,\n    E_global: int,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n    \"\"\"Vectorized token sorting by expert assignment.\"\"\"\n    device = topk_idx.device\n    T, K = topk_idx.shape\n    local_start = local_expert_offset\n    local_end = local_expert_offset + E_local\n    \n    token_indices = torch.arange(T, device=device).unsqueeze(1).expand(T, K)\n    flat_experts = topk_idx.reshape(-1)\n    flat_tokens = token_indices.reshape(-1)\n    \n    local_mask = (flat_experts >= local_start) & (flat_experts < local_end)\n    \n    if not local_mask.any():\n        return (\n            torch.empty(0, dtype=torch.int64, device=device),\n            torch.empty(0, dtype=torch.float32, device=device),\n            torch.zeros(E_local + 1, dtype=torch.int64, device=device),\n            0\n        )\n    \n    local_experts = flat_experts[local_mask] - local_start\n    local_tokens = flat_tokens[local_mask]\n    local_global_experts = flat_experts[local_mask]\n    local_weights = weights[local_tokens, local_global_experts]\n    \n    total_count = local_tokens.shape[0]\n    expert_counts = torch.bincount(local_experts, minlength=E_local)\n    \n    expert_offsets = torch.zeros(E_local + 1, dtype=torch.int64, device=device)\n    expert_offsets[1:] = torch.cumsum(expert_counts, dim=0)\n    \n    sort_idx = torch.argsort(local_experts, stable=True)\n    sorted_token_ids = local_tokens[sort_idx]\n    sorted_weights = local_weights[sort_idx]\n    \n    return sorted_token_ids, sorted_weights, expert_offsets, total_count\n\n\n# ============================================================================\n# CuTile SiLU and Mul Kernel (from TileGym silu_and_mul.py - OPTIMIZED)\n# ============================================================================\n\n@ct.kernel\ndef silu_and_mul_kernel_row_wise(\n    input,\n    output,\n    TILE_SIZE: ConstInt,\n    hidden_size: ConstInt,\n):\n    \"\"\"Optimized fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    bid = ct.bid(0)\n    offsets = ct.arange(TILE_SIZE, dtype=torch.int32)\n\n    row_idx = bid\n    a_col_idx = offsets + hidden_size\n    b_col_idx = offsets\n\n    a_tile = ct.gather(input, (row_idx, a_col_idx), check_bounds=True)\n    b_tile = ct.gather(input, (row_idx, b_col_idx), check_bounds=True)\n    a_tile = ct.astype(a_tile, torch.float32)\n    b_tile = ct.astype(b_tile, torch.float32)\n\n    denom = ct.add(1, ct.exp(-a_tile), flush_to_zero=True)\n    sigmoid_a = ct.truediv(1.0, denom, flush_to_zero=True, rounding_mode=RMd.APPROX)\n    silu_a = ct.mul(a_tile, sigmoid_a, flush_to_zero=True)\n    result = ct.mul(silu_a, b_tile, flush_to_zero=True)\n    result = ct.astype(result, output.dtype)\n\n    out_col_idx = offsets\n    ct.scatter(output, (row_idx, out_col_idx), result, check_bounds=True)\n\n\ndef cutile_silu_and_mul(input: torch.Tensor) -> torch.Tensor:\n    \"\"\"CuTile fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    batch_size, double_hidden = input.shape\n    hidden_size = double_hidden // 2\n    \n    if batch_size == 0:\n        return torch.empty((0, hidden_size), dtype=input.dtype, device=input.device)\n    \n    output = torch.empty((batch_size, hidden_size), dtype=input.dtype, device=input.device)\n    TILE_SIZE = next_power_of_2(hidden_size)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (batch_size,),\n        silu_and_mul_kernel_row_wise,\n        (input.contiguous(), output, TILE_SIZE, hidden_size),\n    )\n    return output\n\n\n# ============================================================================\n# CuTile Group GEMM Kernel - Hardcoded Optimal Tile Sizes\n# ============================================================================\n\n@ct.kernel\ndef group_gemm_kernel(\n    As,\n    Bs,\n    Cs,\n    TILE_M: ConstInt,\n    TILE_N: ConstInt,\n    TILE_K: ConstInt,\n    num_sm: ConstInt,\n    transpose_b: ConstBool,\n):\n    \"\"\"Persistent group GEMM kernel for batching multiple matrix multiplications.\"\"\"\n    tile_idx = ct.bid(0)\n    last_problem_end = 0\n    group_size = len(As)\n    zero_pad = ct.PaddingMode.ZERO\n\n    for g in range(group_size):\n        Ai = As[g]\n        Bi = Bs[g]\n        Ci = Cs[g]\n\n        num_m_tiles = ct.num_tiles(Ai, 0, (TILE_M, TILE_K))\n        num_k_tiles = ct.num_tiles(Ai, 1, (TILE_M, TILE_K))\n        if transpose_b:\n            num_n_tiles = ct.num_tiles(Bi, 0, (TILE_N, TILE_K))\n        else:\n            num_n_tiles = ct.num_tiles(Bi, 1, (TILE_K, TILE_N))\n\n        num_tiles = num_m_tiles * num_n_tiles\n\n        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n            acc = ct.zeros((TILE_M, TILE_N), dtype=ct.float32)\n\n            for kk in range(num_k_tiles):\n                ta = ct.load(Ai, (tile_m_idx, kk), shape=(TILE_M, TILE_K), padding_mode=zero_pad)\n\n                if transpose_b:\n                    tb = ct.load(Bi, (tile_n_idx, kk), shape=(TILE_N, TILE_K), padding_mode=zero_pad)\n                    tb = ct.transpose(tb)\n                else:\n                    tb = ct.load(Bi, (kk, tile_n_idx), shape=(TILE_K, TILE_N), padding_mode=zero_pad)\n\n                ta = ct.astype(ta, ct.tfloat32)\n                tb = ct.astype(tb, ct.tfloat32)\n                acc = ct.mma(ta, tb, acc)\n\n            acc = ct.astype(acc, Ci.dtype)\n            ct.store(Ci, (tile_m_idx, tile_n_idx), tile=acc)\n\n            tile_idx += num_sm\n\n        last_problem_end = last_problem_end + num_tiles\n\n\n# Pre-compiled kernel handles for small and large workloads\n# Note: Both use num_ctas=1 for Blackwell - 128x128 is optimal for large workloads\n_small_kernel = ct.kernel(group_gemm_kernel._pyfunc, num_ctas=1, occupancy=2)\n_large_kernel = ct.kernel(group_gemm_kernel._pyfunc, num_ctas=1, occupancy=1)\n\n\ndef cutile_group_gemm(group_A: List[torch.Tensor], group_B: List[torch.Tensor], transpose_b=True) -> List[torch.Tensor]:\n    \"\"\"CuTile group GEMM with hardcoded optimal tile configurations.\n    \n    Tile sizes determined by benchmarking on NVIDIA B200 (Blackwell):\n    - Small workloads (avg M < 100): TILE_M=64, TILE_N=128, TILE_K=64, occupancy=2\n    - Large workloads (avg M >= 100): TILE_M=128, TILE_N=128, TILE_K=64, occupancy=1\n    Note: 256x256 tiles are MUCH slower on Blackwell (42ms vs 1.7ms for 128x128)\n    \"\"\"\n    if not group_A or not group_B:\n        return []\n    \n    device = group_A[0].device\n    dtype = group_A[0].dtype\n    \n    # Create output tensors\n    group_C = []\n    total_m = 0\n    for A, B in zip(group_A, group_B):\n        M, K = A.shape\n        N = B.shape[0] if transpose_b else B.shape[1]\n        C = torch.empty((M, N), device=device, dtype=dtype)\n        group_C.append(C)\n        total_m += M\n    \n    # Calculate average M dimension per expert\n    avg_m = total_m / len(group_A)\n    \n    NUM_SMS = torch.cuda.get_device_properties(device).multi_processor_count\n    stream = torch.cuda.current_stream()\n    \n    # Prepare contiguous inputs\n    group_A_cont = [a.contiguous() for a in group_A]\n    group_B_cont = [b.contiguous() for b in group_B]\n    \n    # Select tile configuration based on workload size\n    if avg_m < LARGE_WORKLOAD_THRESHOLD:\n        # Small workload config (best for seq_len 1-80)\n        TILE_M, TILE_N, TILE_K = 64, 128, 64\n        num_ctas, occupancy = 1, 2\n        kernel = _small_kernel\n    else:\n        # Large workload config (best for seq_len 901+)\n        # 128x128 is 25x faster than 256x256 on Blackwell!\n        TILE_M, TILE_N, TILE_K = 128, 128, 64\n        num_ctas, occupancy = 1, 1\n        kernel = _large_kernel\n    \n    grid = (NUM_SMS // num_ctas * occupancy, 1, 1)\n    num_sm = NUM_SMS // num_ctas * occupancy\n    \n    ct.launch(\n        stream,\n        grid,\n        kernel,\n        (group_A_cont, group_B_cont, group_C, TILE_M, TILE_N, TILE_K, num_sm, transpose_b),\n    )\n    return group_C\n\n\n# ============================================================================\n# Threshold Configuration\n# ============================================================================\n\nGROUP_GEMM_MIN_EXPERTS = 4\n\n\n# ============================================================================\n# Main Entry Point\n# ============================================================================\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n):\n    \"\"\"\n    MoE kernel v4-hardcoded - CuTile with Hardcoded Optimal Tile Sizes:\n    1. Vectorized token sorting\n    2. CuTile silu_and_mul kernel (optimized activation)\n    3. Group GEMM with hardcoded optimal tile sizes (no autotuning overhead)\n    4. cuBLAS for individual expert GEMMs when few experts active\n    \"\"\"\n    H = 7168\n    I = 2048\n    E_local = gemm1_weights.shape[0]\n    E_global = routing_logits.shape[1]\n    T = routing_logits.shape[0]\n    device = hidden_states.device\n\n    # Step 1: FP8 Dequantization\n    A = dequantize_hidden_states(hidden_states, hidden_states_scale)\n    W13 = dequantize_weights_batched(gemm1_weights, gemm1_weights_scale)\n    W2 = dequantize_weights_batched(gemm2_weights, gemm2_weights_scale)\n\n    # Step 2: DeepSeek-V3 Routing\n    weights, topk_idx = deepseek_routing(\n        routing_logits, routing_bias, routed_scaling_factor\n    )\n\n    # Step 3: Token sorting (vectorized)\n    sorted_token_ids, sorted_weights, expert_offsets, total_count = sort_tokens_by_expert(\n        topk_idx, weights, E_local, local_expert_offset, E_global\n    )\n\n    # Step 4: Expert Computation\n    result = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n    if total_count > 0:\n        expert_counts = expert_offsets[1:] - expert_offsets[:-1]\n        active_experts = (expert_counts > 0).nonzero(as_tuple=True)[0]\n        num_active = len(active_experts)\n        \n        if num_active > 0:\n            expert_token_info = []\n            for le in active_experts.tolist():\n                start = expert_offsets[le].item()\n                end = expert_offsets[le + 1].item()\n                token_idx = sorted_token_ids[start:end]\n                expert_token_info.append((start, end, le, token_idx))\n            \n            if num_active >= GROUP_GEMM_MIN_EXPERTS:\n                group_A = [A[info[3]] for info in expert_token_info]\n                group_W13 = [W13[info[2]] for info in expert_token_info]\n                group_W2 = [W2[info[2]] for info in expert_token_info]\n                \n                gemm1_outputs = cutile_group_gemm(group_A, group_W13, transpose_b=True)\n                swiglu_outputs = [cutile_silu_and_mul(G1) for G1 in gemm1_outputs]\n                gemm2_outputs = cutile_group_gemm(swiglu_outputs, group_W2, transpose_b=True)\n                \n                for i, (start, end, le, token_idx) in enumerate(expert_token_info):\n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, gemm2_outputs[i] * w_tok.unsqueeze(1))\n            else:\n                for start, end, le, token_idx in expert_token_info:\n                    A_e = A[token_idx]\n                    W13_e = W13[le]\n                    W2_e = W2[le]\n                    \n                    G1 = torch.mm(A_e, W13_e.t())\n                    C = cutile_silu_and_mul(G1)\n                    O = torch.mm(C, W2_e.t())\n                    \n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # Step 5: Write output\n    output.copy_(result.to(torch.bfloat16))\n"
    },
    {
      "path": "kernel.py",
      "content": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: MIT\n\n\"\"\"\nMoE Kernel for FlashInfer Competition - DeepSeek-V3 MoE Layer.\n\nVERSION: v3 - CuTile Optimization with Autotuning:\n1. Optimized silu_and_mul kernel from TileGym\n2. Autotuned Group GEMM for batching expert matrix multiplications\n3. Vectorized token sorting\n\"\"\"\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\nimport cuda.tile as ct\nimport torch\nimport random\nimport functools\nimport threading\nimport logging\nfrom contextlib import contextmanager\nfrom math import ceil\nfrom types import SimpleNamespace\nfrom typing import Tuple, List, Any, Iterable, Callable, Sequence\nfrom cuda.tile._numeric_semantics import RoundingMode as RMd\nfrom cuda.tile._exception import TileCompilerTimeoutError, TileCompilerExecutionError\nfrom cuda.tile._cext import default_tile_context\n\nlogger = logging.getLogger(__name__)\n\n# ============================================================================\n# Autotuner (from cuda.tile_experimental)\n# ============================================================================\n\n_MAX_SEARCH_ITEMS = 10_000\n_autotune_lock = threading.RLock()\n\n\ndef _with_autotune_lock(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with _autotune_lock:\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef _shape_dtype_stride(arg: Any):\n    shape = tuple(arg.shape)\n    dtype = arg.dtype\n    stride = None\n    if hasattr(arg, \"stride\"):\n        s = arg.stride() if callable(arg.stride) else arg.stride\n        stride = tuple(int(x) for x in s)\n    elif hasattr(arg, \"strides\"):\n        itemsize = getattr(arg, \"itemsize\", 1)\n        stride = tuple(int(b // itemsize) for b in arg.strides)\n    return shape, dtype, stride\n\n\ndef _default_key(args):\n    tinfo = []\n    for arg in args:\n        if hasattr(arg, \"shape\") and hasattr(arg, \"dtype\"):\n            shape, dtype, stride = _shape_dtype_stride(arg)\n            tinfo.append((shape, dtype, stride))\n        else:\n            tinfo.append(type(arg).__name__)\n    return tuple(tinfo)\n\n\ndef _time_ms(run_once, *, get_args, stream, warmup=2, rep=10):\n    stream.synchronize()\n    for _ in range(warmup):\n        run_once(get_args())\n    args_per_run = [get_args() for _ in range(rep)]\n    stream.synchronize()\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record(stream)\n    for i in range(rep):\n        run_once(args_per_run[i])\n    end.record(stream)\n    end.synchronize()\n    ms = start.elapsed_time(end)\n    return ms / max(1, rep)\n\n\nclass TunedResult:\n    def __init__(self, tuned_config, grid, kernel, tuning_record, cache_hit):\n        self.tuned_config = tuned_config\n        self.grid = grid\n        self.kernel = kernel\n        self.tuning_record = tuning_record\n        self.cache_hit = cache_hit\n\n\nclass _CacheEntry:\n    def __init__(self, best_cfg, best_grid, best_kernel, tuning_record):\n        self.best_cfg = best_cfg\n        self.best_grid = best_grid\n        self.best_kernel = best_kernel\n        self.tuning_record = tuning_record\n\n\n@contextmanager\ndef compiler_timeout(timeout_sec: int):\n    old_timeout = default_tile_context.config.compiler_timeout_sec\n    default_tile_context.config.compiler_timeout_sec = timeout_sec\n    try:\n        yield\n    finally:\n        default_tile_context.config.compiler_timeout_sec = old_timeout\n\n\ndef _reservoir_sample(iterable: Iterable[Any], k: int, *, rng: random.Random, max_items: int) -> list[Any]:\n    reservoir: list[Any] = []\n    n_seen = 0\n    for item in iterable:\n        n_seen += 1\n        if n_seen > max_items:\n            break\n        if len(reservoir) < k:\n            reservoir.append(item)\n        else:\n            j = rng.randint(0, n_seen - 1)\n            if j < k:\n                reservoir[j] = item\n    return reservoir\n\n\ndef autotune_launch(stream, grid_fn, kernel,\n                    args_fn: Callable[[Any], tuple[Any, ...]],\n                    launch_args_fn: Callable[[Any], tuple[Any, ...]] | None = None,\n                    hints_fn: Callable[[Any], dict[str, Any]] | None = None,\n                    *,\n                    search_space: Iterable[Any] | Callable[[], Iterable[Any]],\n                    key: Any | None = None,\n                    max_iter: int = 60,\n                    compiler_time_limit_sec: int = 10,\n                    seed: int | None = None,\n                    force_retune: bool = False) -> TunedResult:\n    if callable(search_space):\n        search_space = search_space()\n\n    rng = random.Random(seed)\n    search_space = _reservoir_sample(search_space, k=max_iter, rng=rng, max_items=_MAX_SEARCH_ITEMS)\n    if len(search_space) == 0:\n        raise ValueError(\"Search space must contain at least 1 configuration\")\n\n    with _autotune_lock:\n        autotune_cache = default_tile_context.autotune_cache\n        if autotune_cache is None:\n            autotune_cache = {}\n            default_tile_context.autotune_cache = autotune_cache\n\n        kernel_key = kernel._pyfunc\n        per_kernel = autotune_cache.get(kernel_key)\n        if per_kernel is None:\n            per_kernel = {}\n            autotune_cache[kernel_key] = per_kernel\n\n        if key is None:\n            arg_key = _default_key(args_fn(search_space[0]))\n        else:\n            arg_key = key\n\n        tuning_entries: list[tuple[Any, float]] = []\n        cache_hit = False\n\n        if not force_retune and arg_key in per_kernel:\n            cache_hit = True\n        else:\n            indices = list(range(len(search_space)))\n            rng.shuffle(indices)\n\n            best_time_ms, best_cfg, best_kernel = float(\"inf\"), None, None\n            for i, cfg_idx in enumerate(indices):\n                cfg = search_space[cfg_idx]\n                grid = grid_fn(cfg)\n                hints = hints_fn(cfg) if hints_fn else {}\n                updated_kernel = ct.kernel(kernel._pyfunc, **hints)\n\n                def run_once(args):\n                    ct.launch(stream, grid, updated_kernel, args)\n\n                try:\n                    with compiler_timeout(compiler_time_limit_sec):\n                        time_ms = _time_ms(run_once, get_args=lambda: args_fn(cfg), stream=stream)\n                except TileCompilerTimeoutError:\n                    continue\n                except TileCompilerExecutionError:\n                    continue\n\n                if time_ms < best_time_ms:\n                    best_time_ms = time_ms\n                    best_cfg, best_grid, best_kernel = cfg, grid, updated_kernel\n                tuning_entries.append((cfg, time_ms))\n\n            if best_cfg is None:\n                raise ValueError(\"No valid config found\")\n            per_kernel[arg_key] = _CacheEntry(best_cfg, best_grid, best_kernel, tuning_entries)\n\n        cache_entry = per_kernel[arg_key]\n\n    best_args = launch_args_fn(cache_entry.best_cfg) if launch_args_fn else args_fn(cache_entry.best_cfg)\n    ct.launch(stream, cache_entry.best_grid, cache_entry.best_kernel, best_args)\n\n    return TunedResult(\n        tuned_config=cache_entry.best_cfg,\n        grid=cache_entry.best_grid,\n        kernel=cache_entry.best_kernel,\n        tuning_record=cache_entry.tuning_record,\n        cache_hit=cache_hit\n    )\n\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\nConstInt = ct.Constant[int]\nConstBool = ct.Constant[bool]\n\n\ndef next_power_of_2(n):\n    \"\"\"Return the next power of 2 >= n.\"\"\"\n    if n <= 0:\n        return 1\n    n -= 1\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    return n + 1\n\n\n# ============================================================================\n# FP8 Dequantization - Fused CuTile Kernels (optimized, no repeat_interleave)\n# ============================================================================\n\n@ct.kernel\ndef dequant_hidden_kernel(\n    data,       # [T, H] FP8\n    scales,     # [num_blocks, T] float32\n    output,     # [T, H] float32\n    TILE_H: ConstInt,\n    H: ConstInt,\n):\n    \"\"\"Fused FP8 dequantization for hidden states - compute scale index on-the-fly.\"\"\"\n    row_idx = ct.bid(0)  # token index\n    col_offsets = ct.arange(TILE_H, dtype=torch.int32)\n    \n    # Load FP8 data for this row\n    data_tile = ct.gather(data, (row_idx, col_offsets), check_bounds=True)\n    data_f32 = ct.astype(data_tile, torch.float32)\n    \n    # Compute block indices: col_offsets // BLOCK -> which scale block\n    # scales is [num_blocks, T], we need scales[block_idx, row_idx]\n    block_indices = col_offsets // 128  # BLOCK = 128\n    \n    # Gather scales for each element based on its block\n    scale_vals = ct.gather(scales, (block_indices, row_idx), check_bounds=True)\n    \n    # Multiply\n    result = ct.mul(data_f32, scale_vals, flush_to_zero=True)\n    \n    ct.scatter(output, (row_idx, col_offsets), result, check_bounds=True)\n\n\ndef dequantize_hidden_states(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 hidden states with block scales to float32 - FUSED.\"\"\"\n    T, H = data.shape\n    \n    if T == 0:\n        return torch.empty((0, H), dtype=torch.float32, device=data.device)\n    \n    output = torch.empty((T, H), dtype=torch.float32, device=data.device)\n    TILE_H = next_power_of_2(H)\n    \n    # scales is [num_blocks, T], keep it as-is (no permute needed)\n    scales_cont = scales.contiguous()\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (T,),  # one block per row/token\n        dequant_hidden_kernel,\n        (data.contiguous(), scales_cont, output, TILE_H, H),\n    )\n    return output\n\n\n@ct.kernel  \ndef dequant_weights_kernel(\n    data,       # [out_dim, in_dim] FP8\n    scales,     # [num_out_blocks, num_in_blocks] float32\n    output,     # [out_dim, in_dim] float32\n    TILE_K: ConstInt,\n    in_dim: ConstInt,\n):\n    \"\"\"Fused FP8 dequantization for weight matrix - compute scale indices on-the-fly.\"\"\"\n    row_idx = ct.bid(0)  # output dimension index\n    col_offsets = ct.arange(TILE_K, dtype=torch.int32)\n    \n    # Load FP8 data for this row\n    data_tile = ct.gather(data, (row_idx, col_offsets), check_bounds=True)\n    data_f32 = ct.astype(data_tile, torch.float32)\n    \n    # Compute block indices\n    row_block = row_idx // 128  # which output block\n    col_blocks = col_offsets // 128  # which input block for each element\n    \n    # Gather scales: scales[row_block, col_block]\n    scale_vals = ct.gather(scales, (row_block, col_blocks), check_bounds=True)\n    \n    # Multiply\n    result = ct.mul(data_f32, scale_vals, flush_to_zero=True)\n    \n    ct.scatter(output, (row_idx, col_offsets), result, check_bounds=True)\n\n\ndef dequantize_weights_batched(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weight matrices with block scales to float32 - FUSED.\"\"\"\n    E_local, out_dim, in_dim = data.shape\n    \n    if E_local == 0:\n        return torch.empty((0, out_dim, in_dim), dtype=torch.float32, device=data.device)\n    \n    output = torch.empty((E_local, out_dim, in_dim), dtype=torch.float32, device=data.device)\n    TILE_K = next_power_of_2(in_dim)\n    \n    # Process each expert separately (could be batched further if needed)\n    for e in range(E_local):\n        data_e = data[e].contiguous()\n        scales_e = scales[e].contiguous()\n        output_e = output[e]\n        \n        ct.launch(\n            torch.cuda.current_stream(),\n            (out_dim,),  # one block per row\n            dequant_weights_kernel,\n            (data_e, scales_e, output_e, TILE_K, in_dim),\n        )\n    \n    return output\n\n\n# ============================================================================\n# Routing\n# ============================================================================\n\ndef deepseek_routing(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    routed_scaling_factor: float,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"DeepSeek-V3 no-aux-loss routing.\"\"\"\n    T, E_global = routing_logits.shape\n    \n    bias = routing_bias.to(torch.float32)\n    s = torch.sigmoid(routing_logits)\n    s_with_bias = s + bias\n    \n    group_size = E_global // N_GROUP\n    s_wb_grouped = s_with_bias.view(T, N_GROUP, group_size)\n    top2_vals, _ = torch.topk(s_wb_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)\n    \n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1, largest=True, sorted=False)\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size).reshape(T, E_global)\n    \n    scores_pruned = torch.where(score_mask > 0, s_with_bias, torch.finfo(torch.float32).min)\n    _, topk_idx = torch.topk(scores_pruned, k=TOP_K, dim=1, largest=True, sorted=False)\n    \n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights_sum = weights.sum(dim=1, keepdim=True) + 1e-20\n    weights = (weights / weights_sum) * routed_scaling_factor\n    \n    return weights, topk_idx\n\n\n# ============================================================================\n# Token Sorting (Vectorized)\n# ============================================================================\n\ndef sort_tokens_by_expert(\n    topk_idx: torch.Tensor,\n    weights: torch.Tensor,\n    E_local: int,\n    local_expert_offset: int,\n    E_global: int,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n    \"\"\"Vectorized token sorting by expert assignment.\"\"\"\n    device = topk_idx.device\n    T, K = topk_idx.shape\n    local_start = local_expert_offset\n    local_end = local_expert_offset + E_local\n    \n    token_indices = torch.arange(T, device=device).unsqueeze(1).expand(T, K)\n    flat_experts = topk_idx.reshape(-1)\n    flat_tokens = token_indices.reshape(-1)\n    \n    local_mask = (flat_experts >= local_start) & (flat_experts < local_end)\n    \n    if not local_mask.any():\n        return (\n            torch.empty(0, dtype=torch.int64, device=device),\n            torch.empty(0, dtype=torch.float32, device=device),\n            torch.zeros(E_local + 1, dtype=torch.int64, device=device),\n            0\n        )\n    \n    local_experts = flat_experts[local_mask] - local_start\n    local_tokens = flat_tokens[local_mask]\n    local_global_experts = flat_experts[local_mask]\n    local_weights = weights[local_tokens, local_global_experts]\n    \n    total_count = local_tokens.shape[0]\n    expert_counts = torch.bincount(local_experts, minlength=E_local)\n    \n    expert_offsets = torch.zeros(E_local + 1, dtype=torch.int64, device=device)\n    expert_offsets[1:] = torch.cumsum(expert_counts, dim=0)\n    \n    sort_idx = torch.argsort(local_experts, stable=True)\n    sorted_token_ids = local_tokens[sort_idx]\n    sorted_weights = local_weights[sort_idx]\n    \n    return sorted_token_ids, sorted_weights, expert_offsets, total_count\n\n\n# ============================================================================\n# CuTile SiLU and Mul Kernel (from TileGym silu_and_mul.py - OPTIMIZED)\n# ============================================================================\n\n@ct.kernel\ndef silu_and_mul_kernel_row_wise(\n    input,\n    output,\n    TILE_SIZE: ConstInt,\n    hidden_size: ConstInt,\n):\n    \"\"\"Optimized fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    bid = ct.bid(0)\n    offsets = ct.arange(TILE_SIZE, dtype=torch.int32)\n\n    row_idx = bid\n    a_col_idx = offsets + hidden_size\n    b_col_idx = offsets\n\n    a_tile = ct.gather(input, (row_idx, a_col_idx), check_bounds=True)\n    b_tile = ct.gather(input, (row_idx, b_col_idx), check_bounds=True)\n    a_tile = ct.astype(a_tile, torch.float32)\n    b_tile = ct.astype(b_tile, torch.float32)\n\n    denom = ct.add(1, ct.exp(-a_tile), flush_to_zero=True)\n    sigmoid_a = ct.truediv(1.0, denom, flush_to_zero=True, rounding_mode=RMd.APPROX)\n    silu_a = ct.mul(a_tile, sigmoid_a, flush_to_zero=True)\n    result = ct.mul(silu_a, b_tile, flush_to_zero=True)\n    result = ct.astype(result, output.dtype)\n\n    out_col_idx = offsets\n    ct.scatter(output, (row_idx, out_col_idx), result, check_bounds=True)\n\n\ndef cutile_silu_and_mul(input: torch.Tensor) -> torch.Tensor:\n    \"\"\"CuTile fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    batch_size, double_hidden = input.shape\n    hidden_size = double_hidden // 2\n    \n    if batch_size == 0:\n        return torch.empty((0, hidden_size), dtype=input.dtype, device=input.device)\n    \n    output = torch.empty((batch_size, hidden_size), dtype=input.dtype, device=input.device)\n    TILE_SIZE = next_power_of_2(hidden_size)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (batch_size,),\n        silu_and_mul_kernel_row_wise,\n        (input.contiguous(), output, TILE_SIZE, hidden_size),\n    )\n    return output\n\n\n# ============================================================================\n# CuTile Group GEMM Kernel with Autotuning (from TileGym group_gemm.py)\n# ============================================================================\n\n@ct.kernel\ndef group_gemm_kernel(\n    As,\n    Bs,\n    Cs,\n    TILE_M: ConstInt,\n    TILE_N: ConstInt,\n    TILE_K: ConstInt,\n    num_sm: ConstInt,\n    transpose_b: ConstBool,\n):\n    \"\"\"Persistent group GEMM kernel for batching multiple matrix multiplications.\"\"\"\n    tile_idx = ct.bid(0)\n    last_problem_end = 0\n    group_size = len(As)\n    zero_pad = ct.PaddingMode.ZERO\n\n    for g in range(group_size):\n        Ai = As[g]\n        Bi = Bs[g]\n        Ci = Cs[g]\n\n        num_m_tiles = ct.num_tiles(Ai, 0, (TILE_M, TILE_K))\n        num_k_tiles = ct.num_tiles(Ai, 1, (TILE_M, TILE_K))\n        if transpose_b:\n            num_n_tiles = ct.num_tiles(Bi, 0, (TILE_N, TILE_K))\n        else:\n            num_n_tiles = ct.num_tiles(Bi, 1, (TILE_K, TILE_N))\n\n        num_tiles = num_m_tiles * num_n_tiles\n\n        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n            acc = ct.zeros((TILE_M, TILE_N), dtype=ct.float32)\n\n            for kk in range(num_k_tiles):\n                ta = ct.load(Ai, (tile_m_idx, kk), shape=(TILE_M, TILE_K), padding_mode=zero_pad)\n\n                if transpose_b:\n                    tb = ct.load(Bi, (tile_n_idx, kk), shape=(TILE_N, TILE_K), padding_mode=zero_pad)\n                    tb = ct.transpose(tb)\n                else:\n                    tb = ct.load(Bi, (kk, tile_n_idx), shape=(TILE_K, TILE_N), padding_mode=zero_pad)\n\n                ta = ct.astype(ta, ct.tfloat32)\n                tb = ct.astype(tb, ct.tfloat32)\n                acc = ct.mma(ta, tb, acc)\n\n            acc = ct.astype(acc, Ci.dtype)\n            ct.store(Ci, (tile_m_idx, tile_n_idx), tile=acc)\n\n            tile_idx += num_sm\n\n        last_problem_end = last_problem_end + num_tiles\n\n\n# ============================================================================\n# Heuristic-Based Config Selection (replaces autotuner for FlashInfer-Bench compatibility)\n# These configs were determined by autotuning on NVIDIA B200 (Blackwell)\n# ============================================================================\n\n# Optimal configs from benchmarking on NVIDIA B200 (Blackwell):\n# - Small workloads (avg M < 100): TILE_M=64, TILE_N=128, TILE_K=64, occupancy=2\n# - Large workloads (avg M >= 100): TILE_M=128, TILE_N=128, TILE_K=64, occupancy=1\n# Note: 256x256 tiles are MUCH slower on Blackwell (42ms vs 1.7ms for 128x128)\nLARGE_WORKLOAD_THRESHOLD = 100\n\n# Pre-defined optimal configurations for Blackwell\n_SMALL_CONFIG = SimpleNamespace(TILE_M=64, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=2)\n_LARGE_CONFIG = SimpleNamespace(TILE_M=128, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=1)\n\n\ndef cutile_group_gemm(group_A: List[torch.Tensor], group_B: List[torch.Tensor], transpose_b=True) -> List[torch.Tensor]:\n    \"\"\"CuTile group GEMM with heuristic-based config selection.\n    \n    Uses optimal tile configurations determined by offline autotuning.\n    Selects config based on workload size for best performance.\n    \"\"\"\n    if not group_A or not group_B:\n        return []\n    \n    device = group_A[0].device\n    dtype = group_A[0].dtype\n    \n    # Create output tensors and calculate total M\n    group_C = []\n    total_m = 0\n    for A, B in zip(group_A, group_B):\n        M, K = A.shape\n        N = B.shape[0] if transpose_b else B.shape[1]\n        C = torch.empty((M, N), device=device, dtype=dtype)\n        group_C.append(C)\n        total_m += M\n    \n    # Select config based on average M dimension\n    avg_m = total_m / len(group_A)\n    cfg = _LARGE_CONFIG if avg_m >= LARGE_WORKLOAD_THRESHOLD else _SMALL_CONFIG\n    \n    NUM_SMS = torch.cuda.get_device_properties(device).multi_processor_count\n    stream = torch.cuda.current_stream()\n    \n    # Prepare contiguous inputs\n    group_A_cont = [a.contiguous() for a in group_A]\n    group_B_cont = [b.contiguous() for b in group_B]\n    \n    # Direct launch with selected config (no autotuning overhead)\n    grid = (NUM_SMS // cfg.num_ctas * cfg.occupancy, 1, 1)\n    num_sm = NUM_SMS // cfg.num_ctas * cfg.occupancy\n    \n    ct.launch(\n        stream,\n        grid,\n        group_gemm_kernel,\n        (group_A_cont, group_B_cont, group_C, \n         cfg.TILE_M, cfg.TILE_N, cfg.TILE_K, num_sm, transpose_b),\n    )\n    return group_C\n\n\n# ============================================================================\n# Threshold Configuration\n# ============================================================================\n\nGROUP_GEMM_MIN_EXPERTS = 4\n\n\n# ============================================================================\n# Main Entry Point\n# ============================================================================\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n):\n    \"\"\"\n    MoE kernel v3 - CuTile Optimization with Autotuning:\n    1. Vectorized token sorting\n    2. CuTile silu_and_mul kernel (optimized activation)\n    3. Autotuned Group GEMM for batching multiple expert GEMMs\n    4. cuBLAS for individual expert GEMMs when few experts active\n    \"\"\"\n    H = 7168\n    I = 2048\n    E_local = gemm1_weights.shape[0]\n    E_global = routing_logits.shape[1]\n    T = routing_logits.shape[0]\n    device = hidden_states.device\n\n    # Step 1: FP8 Dequantization\n    A = dequantize_hidden_states(hidden_states, hidden_states_scale)\n    W13 = dequantize_weights_batched(gemm1_weights, gemm1_weights_scale)\n    W2 = dequantize_weights_batched(gemm2_weights, gemm2_weights_scale)\n\n    # Step 2: DeepSeek-V3 Routing\n    weights, topk_idx = deepseek_routing(\n        routing_logits, routing_bias, routed_scaling_factor\n    )\n\n    # Step 3: Token sorting (vectorized)\n    sorted_token_ids, sorted_weights, expert_offsets, total_count = sort_tokens_by_expert(\n        topk_idx, weights, E_local, local_expert_offset, E_global\n    )\n\n    # Step 4: Expert Computation\n    result = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n    if total_count > 0:\n        expert_counts = expert_offsets[1:] - expert_offsets[:-1]\n        active_experts = (expert_counts > 0).nonzero(as_tuple=True)[0]\n        num_active = len(active_experts)\n        \n        if num_active > 0:\n            expert_token_info = []\n            for le in active_experts.tolist():\n                start = expert_offsets[le].item()\n                end = expert_offsets[le + 1].item()\n                token_idx = sorted_token_ids[start:end]\n                expert_token_info.append((start, end, le, token_idx))\n            \n            if num_active >= GROUP_GEMM_MIN_EXPERTS:\n                group_A = [A[info[3]] for info in expert_token_info]\n                group_W13 = [W13[info[2]] for info in expert_token_info]\n                group_W2 = [W2[info[2]] for info in expert_token_info]\n                \n                gemm1_outputs = cutile_group_gemm(group_A, group_W13, transpose_b=True)\n                swiglu_outputs = [cutile_silu_and_mul(G1) for G1 in gemm1_outputs]\n                gemm2_outputs = cutile_group_gemm(swiglu_outputs, group_W2, transpose_b=True)\n                \n                for i, (start, end, le, token_idx) in enumerate(expert_token_info):\n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, gemm2_outputs[i] * w_tok.unsqueeze(1))\n            else:\n                for start, end, le, token_idx in expert_token_info:\n                    A_e = A[token_idx]\n                    W13_e = W13[le]\n                    W2_e = W2[le]\n                    \n                    G1 = torch.mm(A_e, W13_e.t())\n                    C = cutile_silu_and_mul(G1)\n                    O = torch.mm(C, W2_e.t())\n                    \n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # Step 5: Write output\n    output.copy_(result.to(torch.bfloat16))\n\nif __name__ == \"__main__\":\n    # Constants matching DeepSeek-V3/R1 benchmark spec\n    NUM_EXPERTS = 256\n    NUM_LOCAL_EXPERTS = 32\n    HIDDEN_SIZE = 7168\n    INTERMEDIATE_SIZE = 2048\n    GEMM1_OUT_SIZE = 4096\n    NUM_HIDDEN_BLOCKS = 56      # hidden_size // 128\n    NUM_INTERMEDIATE_BLOCKS = 16  # intermediate_size // 128\n    NUM_GEMM1_OUT_BLOCKS = 32   # gemm1_out_size // 128\n    \n    # Hardcoded seq_len\n    T = 64\n    device = \"cuda\"\n    \n    print(f\"Creating test tensors: seq_len={T}\")\n    print(f\"  Fixed: num_experts={NUM_EXPERTS}, num_local_experts={NUM_LOCAL_EXPERTS}\")\n    print(f\"         hidden_size={HIDDEN_SIZE}, intermediate_size={INTERMEDIATE_SIZE}\")\n    print(f\"         gemm1_out_size={GEMM1_OUT_SIZE}\")\n    \n    # routing_logits: float32 [seq_len, num_experts]\n    routing_logits = torch.randn(T, NUM_EXPERTS, dtype=torch.float32, device=device)\n    \n    # routing_bias: bfloat16 [num_experts]\n    routing_bias = torch.randn(NUM_EXPERTS, dtype=torch.bfloat16, device=device) * 0.1\n    \n    # hidden_states: float8_e4m3fn [seq_len, hidden_size]\n    hidden_states = torch.randn(T, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.5\n    hidden_states = hidden_states.to(torch.float8_e4m3fn)\n    \n    # hidden_states_scale: float32 [num_hidden_blocks, seq_len]\n    hidden_states_scale = torch.ones(NUM_HIDDEN_BLOCKS, T, dtype=torch.float32, device=device) * 0.5\n    \n    # gemm1_weights: float8_e4m3fn [num_local_experts, gemm1_out_size, hidden_size]\n    gemm1_weights = torch.randn(NUM_LOCAL_EXPERTS, GEMM1_OUT_SIZE, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm1_weights = gemm1_weights.to(torch.float8_e4m3fn)\n    \n    # gemm1_weights_scale: float32 [num_local_experts, num_gemm1_out_blocks, num_hidden_blocks]\n    gemm1_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_GEMM1_OUT_BLOCKS, NUM_HIDDEN_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.3\n    \n    # gemm2_weights: float8_e4m3fn [num_local_experts, hidden_size, intermediate_size]\n    gemm2_weights = torch.randn(NUM_LOCAL_EXPERTS, HIDDEN_SIZE, INTERMEDIATE_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm2_weights = gemm2_weights.to(torch.float8_e4m3fn)\n    \n    # gemm2_weights_scale: float32 [num_local_experts, num_hidden_blocks, num_intermediate_blocks]\n    gemm2_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_HIDDEN_BLOCKS, NUM_INTERMEDIATE_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.2\n    \n    # local_expert_offset: int32 scalar\n    local_expert_offset = 0\n    \n    # routed_scaling_factor: float32 scalar\n    routed_scaling_factor = 2.5\n    \n    # Pre-allocate output: bfloat16 [seq_len, hidden_size]\n    output = torch.zeros(T, HIDDEN_SIZE, dtype=torch.bfloat16, device=device)\n    \n    print(f\"Tensors created. GPU Memory: ~{torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    print(\"Running kernel...\")\n    \n    run(\n        routing_logits,\n        routing_bias,\n        hidden_states,\n        hidden_states_scale,\n        gemm1_weights,\n        gemm1_weights_scale,\n        gemm2_weights,\n        gemm2_weights_scale,\n        local_expert_offset,\n        routed_scaling_factor,\n        output,\n    )\n    \n    print(f\"Output shape: {output.shape}, dtype: {output.dtype}\")\n    print(f\"Output stats: min={output.min().item():.4f}, max={output.max().item():.4f}, mean={output.float().mean().item():.4f}\")"
    },
    {
      "path": "noautotune.py",
      "content": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: MIT\n\n\"\"\"\nMoE Kernel for FlashInfer Competition - DeepSeek-V3 MoE Layer.\n\nVERSION: v3-noautotune - CuTile Optimization WITHOUT Autotuning:\n1. Optimized silu_and_mul kernel from TileGym\n2. Group GEMM with fixed tile configuration (no autotuning)\n3. Vectorized token sorting\n\"\"\"\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\nimport cuda.tile as ct\nimport torch\nfrom typing import Tuple, List\nfrom cuda.tile._numeric_semantics import RoundingMode as RMd\n\nConstInt = ct.Constant[int]\nConstBool = ct.Constant[bool]\n\n\ndef next_power_of_2(n):\n    \"\"\"Return the next power of 2 >= n.\"\"\"\n    if n <= 0:\n        return 1\n    n -= 1\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    return n + 1\n\n\n# ============================================================================\n# FP8 Dequantization\n# ============================================================================\n\ndef dequantize_hidden_states(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 hidden states with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_t = scales.permute(1, 0).contiguous()\n    scales_expanded = scales_t.repeat_interleave(BLOCK, dim=1)\n    return data_f32 * scales_expanded\n\n\ndef dequantize_weights_batched(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weight matrices with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_exp = scales.repeat_interleave(BLOCK, dim=1)\n    scales_exp = scales_exp.repeat_interleave(BLOCK, dim=2)\n    return data_f32 * scales_exp\n\n\n# ============================================================================\n# Routing\n# ============================================================================\n\ndef deepseek_routing(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    routed_scaling_factor: float,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"DeepSeek-V3 no-aux-loss routing.\"\"\"\n    T, E_global = routing_logits.shape\n    \n    bias = routing_bias.to(torch.float32)\n    s = torch.sigmoid(routing_logits)\n    s_with_bias = s + bias\n    \n    group_size = E_global // N_GROUP\n    s_wb_grouped = s_with_bias.view(T, N_GROUP, group_size)\n    top2_vals, _ = torch.topk(s_wb_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)\n    \n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1, largest=True, sorted=False)\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size).reshape(T, E_global)\n    \n    scores_pruned = torch.where(score_mask > 0, s_with_bias, torch.finfo(torch.float32).min)\n    _, topk_idx = torch.topk(scores_pruned, k=TOP_K, dim=1, largest=True, sorted=False)\n    \n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights_sum = weights.sum(dim=1, keepdim=True) + 1e-20\n    weights = (weights / weights_sum) * routed_scaling_factor\n    \n    return weights, topk_idx\n\n\n# ============================================================================\n# Token Sorting (Vectorized)\n# ============================================================================\n\ndef sort_tokens_by_expert(\n    topk_idx: torch.Tensor,\n    weights: torch.Tensor,\n    E_local: int,\n    local_expert_offset: int,\n    E_global: int,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n    \"\"\"Vectorized token sorting by expert assignment.\"\"\"\n    device = topk_idx.device\n    T, K = topk_idx.shape\n    local_start = local_expert_offset\n    local_end = local_expert_offset + E_local\n    \n    token_indices = torch.arange(T, device=device).unsqueeze(1).expand(T, K)\n    flat_experts = topk_idx.reshape(-1)\n    flat_tokens = token_indices.reshape(-1)\n    \n    local_mask = (flat_experts >= local_start) & (flat_experts < local_end)\n    \n    if not local_mask.any():\n        return (\n            torch.empty(0, dtype=torch.int64, device=device),\n            torch.empty(0, dtype=torch.float32, device=device),\n            torch.zeros(E_local + 1, dtype=torch.int64, device=device),\n            0\n        )\n    \n    local_experts = flat_experts[local_mask] - local_start\n    local_tokens = flat_tokens[local_mask]\n    local_global_experts = flat_experts[local_mask]\n    local_weights = weights[local_tokens, local_global_experts]\n    \n    total_count = local_tokens.shape[0]\n    expert_counts = torch.bincount(local_experts, minlength=E_local)\n    \n    expert_offsets = torch.zeros(E_local + 1, dtype=torch.int64, device=device)\n    expert_offsets[1:] = torch.cumsum(expert_counts, dim=0)\n    \n    sort_idx = torch.argsort(local_experts, stable=True)\n    sorted_token_ids = local_tokens[sort_idx]\n    sorted_weights = local_weights[sort_idx]\n    \n    return sorted_token_ids, sorted_weights, expert_offsets, total_count\n\n\n# ============================================================================\n# CuTile SiLU and Mul Kernel (from TileGym silu_and_mul.py - OPTIMIZED)\n# ============================================================================\n\n@ct.kernel\ndef silu_and_mul_kernel_row_wise(\n    input,\n    output,\n    TILE_SIZE: ConstInt,\n    hidden_size: ConstInt,\n):\n    \"\"\"Optimized fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    bid = ct.bid(0)\n    offsets = ct.arange(TILE_SIZE, dtype=torch.int32)\n\n    row_idx = bid\n    a_col_idx = offsets + hidden_size\n    b_col_idx = offsets\n\n    a_tile = ct.gather(input, (row_idx, a_col_idx), check_bounds=True)\n    b_tile = ct.gather(input, (row_idx, b_col_idx), check_bounds=True)\n    a_tile = ct.astype(a_tile, torch.float32)\n    b_tile = ct.astype(b_tile, torch.float32)\n\n    denom = ct.add(1, ct.exp(-a_tile), flush_to_zero=True)\n    sigmoid_a = ct.truediv(1.0, denom, flush_to_zero=True, rounding_mode=RMd.APPROX)\n    silu_a = ct.mul(a_tile, sigmoid_a, flush_to_zero=True)\n    result = ct.mul(silu_a, b_tile, flush_to_zero=True)\n    result = ct.astype(result, output.dtype)\n\n    out_col_idx = offsets\n    ct.scatter(output, (row_idx, out_col_idx), result, check_bounds=True)\n\n\ndef cutile_silu_and_mul(input: torch.Tensor) -> torch.Tensor:\n    \"\"\"CuTile fused SiLU and Mul: silu(input[:, hidden:]) * input[:, :hidden]\"\"\"\n    batch_size, double_hidden = input.shape\n    hidden_size = double_hidden // 2\n    \n    if batch_size == 0:\n        return torch.empty((0, hidden_size), dtype=input.dtype, device=input.device)\n    \n    output = torch.empty((batch_size, hidden_size), dtype=input.dtype, device=input.device)\n    TILE_SIZE = next_power_of_2(hidden_size)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (batch_size,),\n        silu_and_mul_kernel_row_wise,\n        (input.contiguous(), output, TILE_SIZE, hidden_size),\n    )\n    return output\n\n\n# ============================================================================\n# CuTile Group GEMM Kernel - Fixed Configuration (NO Autotuning)\n# ============================================================================\n\n@ct.kernel\ndef group_gemm_kernel(\n    As,\n    Bs,\n    Cs,\n    TILE_M: ConstInt,\n    TILE_N: ConstInt,\n    TILE_K: ConstInt,\n    num_sm: ConstInt,\n    transpose_b: ConstBool,\n):\n    \"\"\"Persistent group GEMM kernel for batching multiple matrix multiplications.\"\"\"\n    tile_idx = ct.bid(0)\n    last_problem_end = 0\n    group_size = len(As)\n    zero_pad = ct.PaddingMode.ZERO\n\n    for g in range(group_size):\n        Ai = As[g]\n        Bi = Bs[g]\n        Ci = Cs[g]\n\n        num_m_tiles = ct.num_tiles(Ai, 0, (TILE_M, TILE_K))\n        num_k_tiles = ct.num_tiles(Ai, 1, (TILE_M, TILE_K))\n        if transpose_b:\n            num_n_tiles = ct.num_tiles(Bi, 0, (TILE_N, TILE_K))\n        else:\n            num_n_tiles = ct.num_tiles(Bi, 1, (TILE_K, TILE_N))\n\n        num_tiles = num_m_tiles * num_n_tiles\n\n        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n            acc = ct.zeros((TILE_M, TILE_N), dtype=ct.float32)\n\n            for kk in range(num_k_tiles):\n                ta = ct.load(Ai, (tile_m_idx, kk), shape=(TILE_M, TILE_K), padding_mode=zero_pad)\n\n                if transpose_b:\n                    tb = ct.load(Bi, (tile_n_idx, kk), shape=(TILE_N, TILE_K), padding_mode=zero_pad)\n                    tb = ct.transpose(tb)\n                else:\n                    tb = ct.load(Bi, (kk, tile_n_idx), shape=(TILE_K, TILE_N), padding_mode=zero_pad)\n\n                ta = ct.astype(ta, ct.tfloat32)\n                tb = ct.astype(tb, ct.tfloat32)\n                acc = ct.mma(ta, tb, acc)\n\n            acc = ct.astype(acc, Ci.dtype)\n            ct.store(Ci, (tile_m_idx, tile_n_idx), tile=acc)\n\n            tile_idx += num_sm\n\n        last_problem_end = last_problem_end + num_tiles\n\n\ndef cutile_group_gemm(group_A: List[torch.Tensor], group_B: List[torch.Tensor], transpose_b=True) -> List[torch.Tensor]:\n    \"\"\"CuTile group GEMM with fixed tile configuration (no autotuning).\"\"\"\n    if not group_A or not group_B:\n        return []\n    \n    device = group_A[0].device\n    dtype = group_A[0].dtype\n    \n    # Create output tensors\n    group_C = []\n    for A, B in zip(group_A, group_B):\n        M, K = A.shape\n        N = B.shape[0] if transpose_b else B.shape[1]\n        C = torch.empty((M, N), device=device, dtype=dtype)\n        group_C.append(C)\n    \n    NUM_SMS = torch.cuda.get_device_properties(device).multi_processor_count\n    stream = torch.cuda.current_stream()\n    \n    # Prepare contiguous inputs\n    group_A_cont = [a.contiguous() for a in group_A]\n    group_B_cont = [b.contiguous() for b in group_B]\n    \n    # Fixed tile configuration (no autotuning)\n    # Optimized for seq_len 16-64 (small M dimension per expert)\n    gpu_capability = torch.cuda.get_device_capability()\n    if gpu_capability in [(12, 0), (12, 1)]:\n        # Blackwell configuration - smaller TILE_M for better occupancy with small batches\n        TILE_M, TILE_N, TILE_K = 64, 128, 128\n        num_ctas, occupancy = 1, 1\n    else:\n        # Default configuration for other GPUs (Hopper, Ampere, etc.)\n        TILE_M, TILE_N, TILE_K = 64, 128, 64\n        num_ctas, occupancy = 1, 2\n    \n    grid = (NUM_SMS // num_ctas * occupancy, 1, 1)\n    num_sm = NUM_SMS // num_ctas * occupancy\n    \n    ct.launch(\n        stream,\n        grid,\n        group_gemm_kernel,\n        (group_A_cont, group_B_cont, group_C, TILE_M, TILE_N, TILE_K, num_sm, transpose_b),\n    )\n    return group_C\n\n\n# ============================================================================\n# Threshold Configuration\n# ============================================================================\n\nGROUP_GEMM_MIN_EXPERTS = 4\n\n\n# ============================================================================\n# Main Entry Point\n# ============================================================================\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n):\n    \"\"\"\n    MoE kernel v3-noautotune - CuTile Optimization WITHOUT Autotuning:\n    1. Vectorized token sorting\n    2. CuTile silu_and_mul kernel (optimized activation)\n    3. Group GEMM with fixed tile configuration (no autotuning)\n    4. cuBLAS for individual expert GEMMs when few experts active\n    \"\"\"\n    H = 7168\n    I = 2048\n    E_local = gemm1_weights.shape[0]\n    E_global = routing_logits.shape[1]\n    T = routing_logits.shape[0]\n    device = hidden_states.device\n\n    # Step 1: FP8 Dequantization\n    A = dequantize_hidden_states(hidden_states, hidden_states_scale)\n    W13 = dequantize_weights_batched(gemm1_weights, gemm1_weights_scale)\n    W2 = dequantize_weights_batched(gemm2_weights, gemm2_weights_scale)\n\n    # Step 2: DeepSeek-V3 Routing\n    weights, topk_idx = deepseek_routing(\n        routing_logits, routing_bias, routed_scaling_factor\n    )\n\n    # Step 3: Token sorting (vectorized)\n    sorted_token_ids, sorted_weights, expert_offsets, total_count = sort_tokens_by_expert(\n        topk_idx, weights, E_local, local_expert_offset, E_global\n    )\n\n    # Step 4: Expert Computation\n    result = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n    if total_count > 0:\n        expert_counts = expert_offsets[1:] - expert_offsets[:-1]\n        active_experts = (expert_counts > 0).nonzero(as_tuple=True)[0]\n        num_active = len(active_experts)\n        \n        if num_active > 0:\n            expert_token_info = []\n            for le in active_experts.tolist():\n                start = expert_offsets[le].item()\n                end = expert_offsets[le + 1].item()\n                token_idx = sorted_token_ids[start:end]\n                expert_token_info.append((start, end, le, token_idx))\n            \n            if num_active >= GROUP_GEMM_MIN_EXPERTS:\n                group_A = [A[info[3]] for info in expert_token_info]\n                group_W13 = [W13[info[2]] for info in expert_token_info]\n                group_W2 = [W2[info[2]] for info in expert_token_info]\n                \n                gemm1_outputs = cutile_group_gemm(group_A, group_W13, transpose_b=True)\n                swiglu_outputs = [cutile_silu_and_mul(G1) for G1 in gemm1_outputs]\n                gemm2_outputs = cutile_group_gemm(swiglu_outputs, group_W2, transpose_b=True)\n                \n                for i, (start, end, le, token_idx) in enumerate(expert_token_info):\n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, gemm2_outputs[i] * w_tok.unsqueeze(1))\n            else:\n                for start, end, le, token_idx in expert_token_info:\n                    A_e = A[token_idx]\n                    W13_e = W13[le]\n                    W2_e = W2[le]\n                    \n                    G1 = torch.mm(A_e, W13_e.t())\n                    C = cutile_silu_and_mul(G1)\n                    O = torch.mm(C, W2_e.t())\n                    \n                    w_tok = sorted_weights[start:end]\n                    result.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # Step 5: Write output\n    output.copy_(result.to(torch.bfloat16))\n\nif __name__ == \"__main__\":\n    # Constants matching DeepSeek-V3/R1 benchmark spec\n    NUM_EXPERTS = 256\n    NUM_LOCAL_EXPERTS = 32\n    HIDDEN_SIZE = 7168\n    INTERMEDIATE_SIZE = 2048\n    GEMM1_OUT_SIZE = 4096\n    NUM_HIDDEN_BLOCKS = 56      # hidden_size // 128\n    NUM_INTERMEDIATE_BLOCKS = 16  # intermediate_size // 128\n    NUM_GEMM1_OUT_BLOCKS = 32   # gemm1_out_size // 128\n    \n    # Hardcoded seq_len\n    T = 64\n    device = \"cuda\"\n    \n    print(f\"Creating test tensors: seq_len={T}\")\n    print(f\"  Fixed: num_experts={NUM_EXPERTS}, num_local_experts={NUM_LOCAL_EXPERTS}\")\n    print(f\"         hidden_size={HIDDEN_SIZE}, intermediate_size={INTERMEDIATE_SIZE}\")\n    print(f\"         gemm1_out_size={GEMM1_OUT_SIZE}\")\n    \n    # routing_logits: float32 [seq_len, num_experts]\n    routing_logits = torch.randn(T, NUM_EXPERTS, dtype=torch.float32, device=device)\n    \n    # routing_bias: bfloat16 [num_experts]\n    routing_bias = torch.randn(NUM_EXPERTS, dtype=torch.bfloat16, device=device) * 0.1\n    \n    # hidden_states: float8_e4m3fn [seq_len, hidden_size]\n    hidden_states = torch.randn(T, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.5\n    hidden_states = hidden_states.to(torch.float8_e4m3fn)\n    \n    # hidden_states_scale: float32 [num_hidden_blocks, seq_len]\n    hidden_states_scale = torch.ones(NUM_HIDDEN_BLOCKS, T, dtype=torch.float32, device=device) * 0.5\n    \n    # gemm1_weights: float8_e4m3fn [num_local_experts, gemm1_out_size, hidden_size]\n    gemm1_weights = torch.randn(NUM_LOCAL_EXPERTS, GEMM1_OUT_SIZE, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm1_weights = gemm1_weights.to(torch.float8_e4m3fn)\n    \n    # gemm1_weights_scale: float32 [num_local_experts, num_gemm1_out_blocks, num_hidden_blocks]\n    gemm1_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_GEMM1_OUT_BLOCKS, NUM_HIDDEN_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.3\n    \n    # gemm2_weights: float8_e4m3fn [num_local_experts, hidden_size, intermediate_size]\n    gemm2_weights = torch.randn(NUM_LOCAL_EXPERTS, HIDDEN_SIZE, INTERMEDIATE_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm2_weights = gemm2_weights.to(torch.float8_e4m3fn)\n    \n    # gemm2_weights_scale: float32 [num_local_experts, num_hidden_blocks, num_intermediate_blocks]\n    gemm2_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_HIDDEN_BLOCKS, NUM_INTERMEDIATE_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.2\n    \n    # local_expert_offset: int32 scalar\n    local_expert_offset = 0\n    \n    # routed_scaling_factor: float32 scalar\n    routed_scaling_factor = 2.5\n    \n    # Pre-allocate output: bfloat16 [seq_len, hidden_size]\n    output = torch.zeros(T, HIDDEN_SIZE, dtype=torch.bfloat16, device=device)\n    \n    print(f\"Tensors created. GPU Memory: ~{torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    print(\"Running kernel (no autotuning)...\")\n    \n    run(\n        routing_logits,\n        routing_bias,\n        hidden_states,\n        hidden_states_scale,\n        gemm1_weights,\n        gemm1_weights_scale,\n        gemm2_weights,\n        gemm2_weights_scale,\n        local_expert_offset,\n        routed_scaling_factor,\n        output,\n    )\n    \n    print(f\"Output shape: {output.shape}, dtype: {output.dtype}\")\n    print(f\"Output stats: min={output.min().item():.4f}, max={output.max().item():.4f}, mean={output.float().mean().item():.4f}\")"
    },
    {
      "path": "ok.py",
      "content": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: MIT\n\n\"\"\"\nMoE Kernel for FlashInfer Competition - DeepSeek-V3 MoE Layer.\n\nOptimizations:\n1. Vectorized token sorting\n2. CuTile fused SwiGLU kernel\n\"\"\"\n\nimport cuda.tile as ct\nimport torch\nfrom typing import Tuple\n\nConstInt = ct.Constant[int]\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\n\n# ============================================================================\n# FP8 Dequantization\n# ============================================================================\n\ndef dequantize_hidden_states(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 hidden states with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_t = scales.permute(1, 0).contiguous()\n    scales_expanded = scales_t.repeat_interleave(BLOCK, dim=1)\n    return data_f32 * scales_expanded\n\n\ndef dequantize_weights_batched(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weight matrices with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_exp = scales.repeat_interleave(BLOCK, dim=1)\n    scales_exp = scales_exp.repeat_interleave(BLOCK, dim=2)\n    return data_f32 * scales_exp\n\n\n# ============================================================================\n# Routing\n# ============================================================================\n\ndef deepseek_routing(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    routed_scaling_factor: float,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"DeepSeek-V3 no-aux-loss routing.\"\"\"\n    T, E_global = routing_logits.shape\n    \n    bias = routing_bias.to(torch.float32)\n    s = torch.sigmoid(routing_logits)\n    s_with_bias = s + bias\n    \n    group_size = E_global // N_GROUP\n    s_wb_grouped = s_with_bias.view(T, N_GROUP, group_size)\n    top2_vals, _ = torch.topk(s_wb_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)\n    \n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1, largest=True, sorted=False)\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size).reshape(T, E_global)\n    \n    scores_pruned = torch.where(score_mask > 0, s_with_bias, torch.finfo(torch.float32).min)\n    _, topk_idx = torch.topk(scores_pruned, k=TOP_K, dim=1, largest=True, sorted=False)\n    \n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights_sum = weights.sum(dim=1, keepdim=True) + 1e-20\n    weights = (weights / weights_sum) * routed_scaling_factor\n    \n    return weights, topk_idx\n\n\n# ============================================================================\n# CuTile GEMM Kernel - OPTIMIZATION 3 (for small batches)\n# ============================================================================\n\n@ct.kernel\ndef gemm_kernel(\n    A_ptr,          # [M, K]\n    B_ptr,          # [N, K] (transposed, so we compute A @ B.T)\n    C_ptr,          # [M, N]\n    M: int,\n    N: ConstInt,\n    K: ConstInt,\n    TILE_M: ConstInt,\n    TILE_N: ConstInt,\n    TILE_K: ConstInt,\n):\n    \"\"\"Simple GEMM kernel: C = A @ B.T\"\"\"\n    bid = ct.bid(0)\n    num_n_tiles = ct.cdiv(N, TILE_N)\n    bid_m = bid // num_n_tiles\n    bid_n = bid % num_n_tiles\n    \n    acc = ct.zeros((TILE_M, TILE_N), dtype=ct.float32)\n    num_k_tiles = ct.cdiv(K, TILE_K)\n    \n    for k in range(num_k_tiles):\n        ta = ct.load(A_ptr, index=(bid_m, k), shape=(TILE_M, TILE_K), padding_mode=ct.PaddingMode.ZERO)\n        tb = ct.load(B_ptr, index=(bid_n, k), shape=(TILE_N, TILE_K), padding_mode=ct.PaddingMode.ZERO)\n        tb = ct.transpose(tb)\n        ta = ct.astype(ta, ct.tfloat32)\n        tb = ct.astype(tb, ct.tfloat32)\n        acc = ct.mma(ta, tb, acc)\n    \n    acc = ct.astype(acc, C_ptr.dtype)\n    ct.store(C_ptr, index=(bid_m, bid_n), tile=acc)\n\n\ndef cutile_gemm(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n    \"\"\"CuTile GEMM: C = A @ B.T\"\"\"\n    M, K = A.shape\n    N = B.shape[0]\n    C = torch.empty((M, N), dtype=torch.float32, device=A.device)\n    \n    TILE_M, TILE_N, TILE_K = 64, 64, 64\n    num_m_tiles = (M + TILE_M - 1) // TILE_M\n    num_n_tiles = (N + TILE_N - 1) // TILE_N\n    grid = (num_m_tiles * num_n_tiles,)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        grid,\n        gemm_kernel,\n        (A.contiguous(), B.contiguous(), C, M, N, K, TILE_M, TILE_N, TILE_K),\n    )\n    return C\n\n\n# Threshold: use CuTile GEMM for batch <= this, cuBLAS for larger\nCUTILE_GEMM_THRESHOLD = 64\n\n\n# ============================================================================\n# CuTile Fused SwiGLU Kernel - OPTIMIZATION 2\n# ============================================================================\n\n@ct.kernel\ndef fused_swiglu_kernel(\n    G1_ptr,          # [M, 2*I] - GEMM1 output\n    C_ptr,           # [M, I] - SwiGLU output\n    I: ConstInt,     # intermediate_size\n    TILE_SIZE: ConstInt,\n):\n    \"\"\"Fused SwiGLU: C = silu(G1[:, I:]) * G1[:, :I]\"\"\"\n    bid = ct.bid(0)\n    \n    # Load both halves in one kernel\n    x1 = ct.load(G1_ptr, index=(bid, 0), shape=(1, TILE_SIZE), padding_mode=ct.PaddingMode.ZERO)\n    x2 = ct.load(G1_ptr, index=(bid, 1), shape=(1, TILE_SIZE), padding_mode=ct.PaddingMode.ZERO)\n    \n    x1_f32 = ct.astype(x1, ct.float32)\n    x2_f32 = ct.astype(x2, ct.float32)\n    \n    # SiLU(x2) = x2 * sigmoid(x2)\n    sigmoid_x2 = 1.0 / (1.0 + ct.exp(-x2_f32))\n    silu_x2 = x2_f32 * sigmoid_x2\n    \n    # Result = silu(x2) * x1\n    result = silu_x2 * x1_f32\n    result = ct.astype(result, C_ptr.dtype)\n    \n    ct.store(C_ptr, index=(bid, 0), tile=result)\n\n\ndef cutile_swiglu(G1: torch.Tensor, I: int) -> torch.Tensor:\n    \"\"\"CuTile fused SwiGLU: silu(G1[:, I:]) * G1[:, :I]\"\"\"\n    M = G1.shape[0]\n    if M == 0:\n        return torch.empty((0, I), dtype=G1.dtype, device=G1.device)\n    C = torch.empty((M, I), dtype=G1.dtype, device=G1.device)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (M,),\n        fused_swiglu_kernel,\n        (G1.contiguous(), C, I, I),\n    )\n    return C\n\n\n# ============================================================================\n# Token Sorting (Vectorized) - OPTIMIZATION 1\n# ============================================================================\n\ndef sort_tokens_by_expert(\n    topk_idx: torch.Tensor,     # [T, TOP_K]\n    weights: torch.Tensor,      # [T, E_global]\n    E_local: int,\n    local_expert_offset: int,\n    E_global: int,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n    \"\"\"Vectorized token sorting by expert assignment.\"\"\"\n    device = topk_idx.device\n    T, K = topk_idx.shape\n    local_start = local_expert_offset\n    local_end = local_expert_offset + E_local\n    \n    token_indices = torch.arange(T, device=device).unsqueeze(1).expand(T, K)\n    flat_experts = topk_idx.reshape(-1)\n    flat_tokens = token_indices.reshape(-1)\n    \n    local_mask = (flat_experts >= local_start) & (flat_experts < local_end)\n    \n    if not local_mask.any():\n        return (\n            torch.empty(0, dtype=torch.int64, device=device),\n            torch.empty(0, dtype=torch.float32, device=device),\n            torch.zeros(E_local + 1, dtype=torch.int64, device=device),\n            0\n        )\n    \n    local_experts = flat_experts[local_mask] - local_start\n    local_tokens = flat_tokens[local_mask]\n    local_global_experts = flat_experts[local_mask]\n    local_weights = weights[local_tokens, local_global_experts]\n    \n    total_count = local_tokens.shape[0]\n    expert_counts = torch.bincount(local_experts, minlength=E_local)\n    \n    expert_offsets = torch.zeros(E_local + 1, dtype=torch.int64, device=device)\n    expert_offsets[1:] = torch.cumsum(expert_counts, dim=0)\n    \n    sort_idx = torch.argsort(local_experts, stable=True)\n    sorted_token_ids = local_tokens[sort_idx]\n    sorted_weights = local_weights[sort_idx]\n    \n    return sorted_token_ids, sorted_weights, expert_offsets, total_count\n\n\n# ============================================================================\n# Main Entry Point\n# ============================================================================\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n):\n    \"\"\"\n    Optimized MoE kernel.\n    Optimizations:\n    1. Vectorized token sorting\n    2. CuTile fused SwiGLU kernel (batched)\n    3. CuTile GEMM for small batches\n    \"\"\"\n    H = 7168\n    I = 2048\n    E_local = gemm1_weights.shape[0]\n    E_global = routing_logits.shape[1]\n    T = routing_logits.shape[0]\n    device = hidden_states.device\n\n    # Step 1: FP8 Dequantization\n    A = dequantize_hidden_states(hidden_states, hidden_states_scale)\n    W13 = dequantize_weights_batched(gemm1_weights, gemm1_weights_scale)\n    W2 = dequantize_weights_batched(gemm2_weights, gemm2_weights_scale)\n\n    # Step 2: DeepSeek-V3 Routing\n    weights, topk_idx = deepseek_routing(\n        routing_logits, routing_bias, routed_scaling_factor\n    )\n\n    # Step 3: Token sorting (OPTIMIZATION 1)\n    sorted_token_ids, sorted_weights, expert_offsets, total_count = sort_tokens_by_expert(\n        topk_idx, weights, E_local, local_expert_offset, E_global\n    )\n\n    # Step 4: Expert Computation with Batched SwiGLU (OPTIMIZATION 3)\n    result = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n    if total_count > 0:\n        expert_counts = expert_offsets[1:] - expert_offsets[:-1]\n        active_experts = (expert_counts > 0).nonzero(as_tuple=True)[0]\n        \n        # Phase 1: GEMM1 for all experts\n        G1_list = []\n        expert_info = []\n        \n        for le in active_experts.tolist():\n            start = expert_offsets[le].item()\n            end = expert_offsets[le + 1].item()\n            \n            token_idx = sorted_token_ids[start:end]\n            w_tok = sorted_weights[start:end]\n            A_e = A[token_idx]\n            W13_e = W13[le]\n            batch_size = A_e.shape[0]\n            \n            # OPTIMIZATION 3: CuTile GEMM for small batches\n            if batch_size <= CUTILE_GEMM_THRESHOLD:\n                G1 = cutile_gemm(A_e, W13_e)\n            else:\n                G1 = torch.mm(A_e, W13_e.t())\n            G1_list.append(G1)\n            expert_info.append((le, token_idx, w_tok, G1.shape[0]))\n        \n        # Phase 2: Batched SwiGLU (single kernel call)\n        if G1_list:\n            G1_cat = torch.cat(G1_list, dim=0)\n            C_cat = cutile_swiglu(G1_cat, I)\n            \n            # Phase 3: GEMM2 for all experts\n            offset = 0\n            for le, token_idx, w_tok, size in expert_info:\n                C = C_cat[offset:offset + size]\n                offset += size\n                \n                W2_e = W2[le]\n                # OPTIMIZATION 3: CuTile GEMM for small batches\n                if size <= CUTILE_GEMM_THRESHOLD:\n                    O = cutile_gemm(C, W2_e)\n                else:\n                    O = torch.mm(C, W2_e.t())\n                \n                result.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # Step 5: Write output (DPS)\n    output.copy_(result.to(torch.bfloat16))"
    },
    {
      "path": "smallkernel.py",
      "content": "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: MIT\n\n\"\"\"\nMoE Kernel for FlashInfer Competition - DeepSeek-V3 MoE Layer.\n\nOPTIMIZED FOR SMALL SEQUENCE LENGTHS (1-100 tokens):\n- All experts use CuTile GEMM (no cuBLAS fallback)\n- Smaller tile sizes for better occupancy on small batches\n- Aggressive autotuning for small M dimensions\n- Batched SwiGLU for reduced kernel launch overhead\n\"\"\"\n\nfrom __future__ import annotations\nimport cuda.tile as ct\nimport torch\nimport random\nimport functools\nimport threading\nimport logging\nfrom contextlib import contextmanager\nfrom typing import Tuple, Any, Iterable, Callable, Sequence\nfrom types import SimpleNamespace\nfrom cuda.tile._exception import TileCompilerTimeoutError, TileCompilerExecutionError\nfrom cuda.tile._cext import default_tile_context\n\nlogger = logging.getLogger(__name__)\n\nConstInt = ct.Constant[int]\n\n# ============================================================================\n# Autotuner (from cuda.tile_experimental)\n# ============================================================================\n\n_MAX_SEARCH_ITEMS = 10_000\n_autotune_lock = threading.RLock()\n\n\ndef _with_autotune_lock(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with _autotune_lock:\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef _shape_dtype_stride(arg: Any):\n    shape = tuple(arg.shape)\n    dtype = arg.dtype\n    stride = None\n    if hasattr(arg, \"stride\"):\n        s = arg.stride() if callable(arg.stride) else arg.stride\n        stride = tuple(int(x) for x in s)\n    elif hasattr(arg, \"strides\"):\n        itemsize = getattr(arg, \"itemsize\", 1)\n        stride = tuple(int(b // itemsize) for b in arg.strides)\n    return shape, dtype, stride\n\n\ndef _default_key(args):\n    tinfo = []\n    for arg in args:\n        if hasattr(arg, \"shape\") and hasattr(arg, \"dtype\"):\n            shape, dtype, stride = _shape_dtype_stride(arg)\n            tinfo.append((shape, dtype, stride))\n        else:\n            tinfo.append(type(arg).__name__)\n    return tuple(tinfo)\n\n\ndef _time_ms(run_once, *, get_args, stream, warmup=2, rep=10):\n    stream.synchronize()\n    for _ in range(warmup):\n        run_once(get_args())\n    args_per_run = [get_args() for _ in range(rep)]\n    stream.synchronize()\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record(stream)\n    for i in range(rep):\n        run_once(args_per_run[i])\n    end.record(stream)\n    end.synchronize()\n    ms = start.elapsed_time(end)\n    return ms / max(1, rep)\n\n\nclass TunedResult:\n    def __init__(self, tuned_config, grid, kernel, tuning_record, cache_hit):\n        self.tuned_config = tuned_config\n        self.grid = grid\n        self.kernel = kernel\n        self.tuning_record = tuning_record\n        self.cache_hit = cache_hit\n\n\nclass _CacheEntry:\n    def __init__(self, best_cfg, best_grid, best_kernel, tuning_record):\n        self.best_cfg = best_cfg\n        self.best_grid = best_grid\n        self.best_kernel = best_kernel\n        self.tuning_record = tuning_record\n\n\n@contextmanager\ndef compiler_timeout(timeout_sec: int):\n    old_timeout = default_tile_context.config.compiler_timeout_sec\n    default_tile_context.config.compiler_timeout_sec = timeout_sec\n    try:\n        yield\n    finally:\n        default_tile_context.config.compiler_timeout_sec = old_timeout\n\n\ndef _reservoir_sample(iterable: Iterable[Any], k: int, *, rng: random.Random, max_items: int):\n    reservoir = []\n    n_seen = 0\n    for item in iterable:\n        n_seen += 1\n        if n_seen > max_items:\n            break\n        if len(reservoir) < k:\n            reservoir.append(item)\n        else:\n            j = rng.randint(0, n_seen - 1)\n            if j < k:\n                reservoir[j] = item\n    return reservoir\n\n\ndef autotune_launch(stream, grid_fn, kernel,\n                    args_fn: Callable[[Any], tuple[Any, ...]],\n                    launch_args_fn: Callable[[Any], tuple[Any, ...]] | None = None,\n                    hints_fn: Callable[[Any], dict[str, Any]] | None = None,\n                    *,\n                    search_space: Iterable[Any] | Callable[[], Iterable[Any]],\n                    key: Any | None = None,\n                    max_iter: int = 60,\n                    compiler_time_limit_sec: int = 10,\n                    seed: int | None = None,\n                    force_retune: bool = False) -> TunedResult:\n    if callable(search_space):\n        search_space = search_space()\n\n    rng = random.Random(seed)\n    search_space = _reservoir_sample(search_space, k=max_iter, rng=rng, max_items=_MAX_SEARCH_ITEMS)\n    if len(search_space) == 0:\n        raise ValueError(\"Search space must contain at least 1 configuration\")\n\n    with _autotune_lock:\n        autotune_cache = default_tile_context.autotune_cache\n        if autotune_cache is None:\n            autotune_cache = {}\n            default_tile_context.autotune_cache = autotune_cache\n\n        kernel_key = kernel._pyfunc\n        per_kernel = autotune_cache.get(kernel_key)\n        if per_kernel is None:\n            per_kernel = {}\n            autotune_cache[kernel_key] = per_kernel\n\n        if key is None:\n            arg_key = _default_key(args_fn(search_space[0]))\n        else:\n            arg_key = key\n\n        tuning_entries = []\n        cache_hit = False\n\n        if not force_retune and arg_key in per_kernel:\n            cache_hit = True\n        else:\n            indices = list(range(len(search_space)))\n            rng.shuffle(indices)\n\n            best_time_ms, best_cfg, best_kernel = float(\"inf\"), None, None\n            for i, cfg_idx in enumerate(indices):\n                cfg = search_space[cfg_idx]\n                grid = grid_fn(cfg)\n                hints = hints_fn(cfg) if hints_fn else {}\n                updated_kernel = ct.kernel(kernel._pyfunc, **hints)\n\n                def run_once(args):\n                    ct.launch(stream, grid, updated_kernel, args)\n\n                try:\n                    with compiler_timeout(compiler_time_limit_sec):\n                        time_ms = _time_ms(run_once, get_args=lambda: args_fn(cfg), stream=stream)\n                except TileCompilerTimeoutError:\n                    continue\n                except TileCompilerExecutionError:\n                    continue\n\n                if time_ms < best_time_ms:\n                    best_time_ms = time_ms\n                    best_cfg, best_grid, best_kernel = cfg, grid, updated_kernel\n                tuning_entries.append((cfg, time_ms))\n\n            if best_cfg is None:\n                raise ValueError(\"No valid config found\")\n            per_kernel[arg_key] = _CacheEntry(best_cfg, best_grid, best_kernel, tuning_entries)\n\n        cache_entry = per_kernel[arg_key]\n\n    best_args = launch_args_fn(cache_entry.best_cfg) if launch_args_fn else args_fn(cache_entry.best_cfg)\n    ct.launch(stream, cache_entry.best_grid, cache_entry.best_kernel, best_args)\n\n    return TunedResult(\n        tuned_config=cache_entry.best_cfg,\n        grid=cache_entry.best_grid,\n        kernel=cache_entry.best_kernel,\n        tuning_record=cache_entry.tuning_record,\n        cache_hit=cache_hit\n    )\n\n\n# ============================================================================\n# Constants for DeepSeek-V3/R1 MoE\n# ============================================================================\nBLOCK = 128  # FP8 block scale block size\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\n\n\n# ============================================================================\n# FP8 Dequantization\n# ============================================================================\n\ndef dequantize_hidden_states(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 hidden states with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_t = scales.permute(1, 0).contiguous()\n    scales_expanded = scales_t.repeat_interleave(BLOCK, dim=1)\n    return data_f32 * scales_expanded\n\n\ndef dequantize_weights_batched(data: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:\n    \"\"\"Dequantize FP8 weight matrices with block scales to float32.\"\"\"\n    data_f32 = data.to(torch.float32)\n    scales_exp = scales.repeat_interleave(BLOCK, dim=1)\n    scales_exp = scales_exp.repeat_interleave(BLOCK, dim=2)\n    return data_f32 * scales_exp\n\n\n# ============================================================================\n# Routing\n# ============================================================================\n\ndef deepseek_routing(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    routed_scaling_factor: float,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"DeepSeek-V3 no-aux-loss routing.\"\"\"\n    T, E_global = routing_logits.shape\n    \n    bias = routing_bias.to(torch.float32)\n    s = torch.sigmoid(routing_logits)\n    s_with_bias = s + bias\n    \n    group_size = E_global // N_GROUP\n    s_wb_grouped = s_with_bias.view(T, N_GROUP, group_size)\n    top2_vals, _ = torch.topk(s_wb_grouped, k=2, dim=2, largest=True, sorted=False)\n    group_scores = top2_vals.sum(dim=2)\n    \n    _, group_idx = torch.topk(group_scores, k=TOPK_GROUP, dim=1, largest=True, sorted=False)\n    group_mask = torch.zeros_like(group_scores)\n    group_mask.scatter_(1, group_idx, 1.0)\n    score_mask = group_mask.unsqueeze(2).expand(T, N_GROUP, group_size).reshape(T, E_global)\n    \n    scores_pruned = torch.where(score_mask > 0, s_with_bias, torch.finfo(torch.float32).min)\n    _, topk_idx = torch.topk(scores_pruned, k=TOP_K, dim=1, largest=True, sorted=False)\n    \n    M = torch.zeros_like(s)\n    M.scatter_(1, topk_idx, 1.0)\n    weights = s * M\n    weights_sum = weights.sum(dim=1, keepdim=True) + 1e-20\n    weights = (weights / weights_sum) * routed_scaling_factor\n    \n    return weights, topk_idx\n\n\n# ============================================================================\n# CuTile GEMM Kernel with Autotuning - OPTIMIZED FOR SMALL M\n# ============================================================================\n\n@ct.kernel\ndef small_gemm_kernel(\n    A_ptr,          # [M, K]\n    B_ptr,          # [N, K] (transposed, so we compute A @ B.T)\n    C_ptr,          # [M, N]\n    M: int,\n    N: ConstInt,\n    K: ConstInt,\n    TILE_M: ConstInt,\n    TILE_N: ConstInt,\n    TILE_K: ConstInt,\n):\n    \"\"\"GEMM kernel optimized for small M: C = A @ B.T\"\"\"\n    bid = ct.bid(0)\n    num_n_tiles = ct.cdiv(N, TILE_N)\n    bid_m = bid // num_n_tiles\n    bid_n = bid % num_n_tiles\n    \n    acc = ct.zeros((TILE_M, TILE_N), dtype=ct.float32)\n    num_k_tiles = ct.cdiv(K, TILE_K)\n    \n    for k in range(num_k_tiles):\n        ta = ct.load(A_ptr, index=(bid_m, k), shape=(TILE_M, TILE_K), padding_mode=ct.PaddingMode.ZERO)\n        tb = ct.load(B_ptr, index=(bid_n, k), shape=(TILE_N, TILE_K), padding_mode=ct.PaddingMode.ZERO)\n        tb = ct.transpose(tb)\n        ta = ct.astype(ta, ct.tfloat32)\n        tb = ct.astype(tb, ct.tfloat32)\n        acc = ct.mma(ta, tb, acc)\n    \n    acc = ct.astype(acc, C_ptr.dtype)\n    ct.store(C_ptr, index=(bid_m, bid_n), tile=acc)\n\n\ndef _small_gemm_autotune_configs():\n    \"\"\"Autotune configurations for small M GEMM - smaller tiles for better occupancy.\"\"\"\n    gpu_capability = torch.cuda.get_device_capability()\n    if gpu_capability in [(12, 0), (12, 1)]:\n        # sm120, sm121 - smaller tiles for small batches\n        yield SimpleNamespace(TILE_M=16, TILE_N=64, TILE_K=64, num_ctas=1, occupancy=4)\n        yield SimpleNamespace(TILE_M=32, TILE_N=64, TILE_K=64, num_ctas=1, occupancy=4)\n        yield SimpleNamespace(TILE_M=16, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=2)\n        yield SimpleNamespace(TILE_M=32, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=2)\n        yield SimpleNamespace(TILE_M=64, TILE_N=64, TILE_K=64, num_ctas=1, occupancy=2)\n    else:\n        # Blackwell - smaller tiles for small batches\n        yield SimpleNamespace(TILE_M=16, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=4)\n        yield SimpleNamespace(TILE_M=32, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=2)\n        yield SimpleNamespace(TILE_M=32, TILE_N=256, TILE_K=64, num_ctas=1, occupancy=2)\n        yield SimpleNamespace(TILE_M=64, TILE_N=128, TILE_K=64, num_ctas=1, occupancy=2)\n        yield SimpleNamespace(TILE_M=64, TILE_N=256, TILE_K=64, num_ctas=1, occupancy=1)\n\n\ndef cutile_small_gemm(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n    \"\"\"CuTile GEMM optimized for small M: C = A @ B.T\"\"\"\n    M, K = A.shape\n    N = B.shape[0]\n    C = torch.empty((M, N), dtype=torch.float32, device=A.device)\n    \n    stream = torch.cuda.current_stream()\n    A_cont = A.contiguous()\n    B_cont = B.contiguous()\n    \n    autotune_launch(\n        stream,\n        grid_fn=lambda cfg: (((M + cfg.TILE_M - 1) // cfg.TILE_M) * ((N + cfg.TILE_N - 1) // cfg.TILE_N),),\n        kernel=small_gemm_kernel,\n        args_fn=lambda cfg: (A_cont, B_cont, C, M, N, K, cfg.TILE_M, cfg.TILE_N, cfg.TILE_K),\n        hints_fn=lambda cfg: {\"num_ctas\": cfg.num_ctas, \"occupancy\": cfg.occupancy},\n        search_space=_small_gemm_autotune_configs,\n    )\n    return C\n\n\n# ============================================================================\n# CuTile Fused SwiGLU Kernel\n# ============================================================================\n\n@ct.kernel\ndef fused_swiglu_kernel(\n    G1_ptr,          # [M, 2*I] - GEMM1 output\n    C_ptr,           # [M, I] - SwiGLU output\n    I: ConstInt,     # intermediate_size\n    TILE_SIZE: ConstInt,\n):\n    \"\"\"Fused SwiGLU: C = silu(G1[:, I:]) * G1[:, :I]\"\"\"\n    bid = ct.bid(0)\n    \n    # Load both halves in one kernel\n    x1 = ct.load(G1_ptr, index=(bid, 0), shape=(1, TILE_SIZE), padding_mode=ct.PaddingMode.ZERO)\n    x2 = ct.load(G1_ptr, index=(bid, 1), shape=(1, TILE_SIZE), padding_mode=ct.PaddingMode.ZERO)\n    \n    x1_f32 = ct.astype(x1, ct.float32)\n    x2_f32 = ct.astype(x2, ct.float32)\n    \n    # SiLU(x2) = x2 * sigmoid(x2)\n    sigmoid_x2 = 1.0 / (1.0 + ct.exp(-x2_f32))\n    silu_x2 = x2_f32 * sigmoid_x2\n    \n    # Result = silu(x2) * x1\n    result = silu_x2 * x1_f32\n    result = ct.astype(result, C_ptr.dtype)\n    \n    ct.store(C_ptr, index=(bid, 0), tile=result)\n\n\ndef cutile_swiglu(G1: torch.Tensor, I: int) -> torch.Tensor:\n    \"\"\"CuTile fused SwiGLU: silu(G1[:, I:]) * G1[:, :I]\"\"\"\n    M = G1.shape[0]\n    if M == 0:\n        return torch.empty((0, I), dtype=G1.dtype, device=G1.device)\n    C = torch.empty((M, I), dtype=G1.dtype, device=G1.device)\n    \n    ct.launch(\n        torch.cuda.current_stream(),\n        (M,),\n        fused_swiglu_kernel,\n        (G1.contiguous(), C, I, I),\n    )\n    return C\n\n\n# ============================================================================\n# Token Sorting (Vectorized)\n# ============================================================================\n\ndef sort_tokens_by_expert(\n    topk_idx: torch.Tensor,     # [T, TOP_K]\n    weights: torch.Tensor,      # [T, E_global]\n    E_local: int,\n    local_expert_offset: int,\n    E_global: int,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n    \"\"\"Vectorized token sorting by expert assignment.\"\"\"\n    device = topk_idx.device\n    T, K = topk_idx.shape\n    local_start = local_expert_offset\n    local_end = local_expert_offset + E_local\n    \n    token_indices = torch.arange(T, device=device).unsqueeze(1).expand(T, K)\n    flat_experts = topk_idx.reshape(-1)\n    flat_tokens = token_indices.reshape(-1)\n    \n    local_mask = (flat_experts >= local_start) & (flat_experts < local_end)\n    \n    if not local_mask.any():\n        return (\n            torch.empty(0, dtype=torch.int64, device=device),\n            torch.empty(0, dtype=torch.float32, device=device),\n            torch.zeros(E_local + 1, dtype=torch.int64, device=device),\n            0\n        )\n    \n    local_experts = flat_experts[local_mask] - local_start\n    local_tokens = flat_tokens[local_mask]\n    local_global_experts = flat_experts[local_mask]\n    local_weights = weights[local_tokens, local_global_experts]\n    \n    total_count = local_tokens.shape[0]\n    expert_counts = torch.bincount(local_experts, minlength=E_local)\n    \n    expert_offsets = torch.zeros(E_local + 1, dtype=torch.int64, device=device)\n    expert_offsets[1:] = torch.cumsum(expert_counts, dim=0)\n    \n    sort_idx = torch.argsort(local_experts, stable=True)\n    sorted_token_ids = local_tokens[sort_idx]\n    sorted_weights = local_weights[sort_idx]\n    \n    return sorted_token_ids, sorted_weights, expert_offsets, total_count\n\n\n# ============================================================================\n# Main Entry Point - OPTIMIZED FOR SMALL SEQUENCE LENGTHS\n# ============================================================================\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n):\n    \"\"\"\n    MoE kernel OPTIMIZED FOR SMALL SEQUENCE LENGTHS (1-100 tokens):\n    1. Always uses CuTile GEMM (no cuBLAS fallback)\n    2. Smaller tile sizes for better occupancy\n    3. Batched SwiGLU for reduced kernel launch overhead\n    \"\"\"\n    H = 7168\n    I = 2048\n    E_local = gemm1_weights.shape[0]\n    E_global = routing_logits.shape[1]\n    T = routing_logits.shape[0]\n    device = hidden_states.device\n\n    # Step 1: FP8 Dequantization\n    A = dequantize_hidden_states(hidden_states, hidden_states_scale)\n    W13 = dequantize_weights_batched(gemm1_weights, gemm1_weights_scale)\n    W2 = dequantize_weights_batched(gemm2_weights, gemm2_weights_scale)\n\n    # Step 2: DeepSeek-V3 Routing\n    weights, topk_idx = deepseek_routing(\n        routing_logits, routing_bias, routed_scaling_factor\n    )\n\n    # Step 3: Token sorting (vectorized)\n    sorted_token_ids, sorted_weights, expert_offsets, total_count = sort_tokens_by_expert(\n        topk_idx, weights, E_local, local_expert_offset, E_global\n    )\n\n    # Step 4: Expert Computation - ALL CuTile for small batches\n    result = torch.zeros((T, H), dtype=torch.float32, device=device)\n\n    if total_count > 0:\n        expert_counts = expert_offsets[1:] - expert_offsets[:-1]\n        active_experts = (expert_counts > 0).nonzero(as_tuple=True)[0]\n        \n        # Phase 1: GEMM1 for all experts using CuTile\n        G1_list = []\n        expert_info = []\n        \n        for le in active_experts.tolist():\n            start = expert_offsets[le].item()\n            end = expert_offsets[le + 1].item()\n            \n            token_idx = sorted_token_ids[start:end]\n            w_tok = sorted_weights[start:end]\n            A_e = A[token_idx]\n            W13_e = W13[le]\n            \n            # Always use CuTile GEMM for small batches\n            G1 = cutile_small_gemm(A_e, W13_e)\n            G1_list.append(G1)\n            expert_info.append((le, token_idx, w_tok, G1.shape[0]))\n        \n        # Phase 2: Batched SwiGLU (single kernel call for all tokens)\n        if G1_list:\n            G1_cat = torch.cat(G1_list, dim=0)\n            C_cat = cutile_swiglu(G1_cat, I)\n            \n            # Phase 3: GEMM2 for all experts using CuTile\n            offset = 0\n            for le, token_idx, w_tok, size in expert_info:\n                C = C_cat[offset:offset + size]\n                offset += size\n                \n                W2_e = W2[le]\n                # Always use CuTile GEMM for small batches\n                O = cutile_small_gemm(C, W2_e)\n                \n                result.index_add_(0, token_idx, O * w_tok.unsqueeze(1))\n\n    # Step 5: Write output (DPS)\n    output.copy_(result.to(torch.bfloat16))\n"
    },
    {
      "path": "test_cutile.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nQuick test script to compare CuTile vs PyTorch MoE implementations.\n\nUsage:\n    python test_cutile.py                    # Run correctness test\n    python test_cutile.py --benchmark        # Run benchmark\n    python test_cutile.py --benchmark --iterations 100\n\"\"\"\n\nimport argparse\nimport time\nfrom typing import Tuple\n\nimport torch\n\n# ============================================================================\n# Constants matching DeepSeek-V3/R1 benchmark spec (fixed values)\n# ============================================================================\n# Geometry\nNUM_EXPERTS = 256           # num_experts (E_global)\nNUM_LOCAL_EXPERTS = 32      # num_local_experts (E_local)\nHIDDEN_SIZE = 7168          # hidden_size (H)\nINTERMEDIATE_SIZE = 2048    # intermediate_size (I)\nGEMM1_OUT_SIZE = 4096       # gemm1_out_size (2*I)\n\n# Block sizes for FP8 quantization (block = 128)\nBLOCK = 128\nNUM_HIDDEN_BLOCKS = 56      # hidden_size // 128 = 7168 // 128\nNUM_INTERMEDIATE_BLOCKS = 16  # intermediate_size // 128 = 2048 // 128\nNUM_GEMM1_OUT_BLOCKS = 32   # gemm1_out_size // 128 = 4096 // 128\n\n# Aliases for backward compatibility\nH = HIDDEN_SIZE\nI = INTERMEDIATE_SIZE\nE_LOCAL = NUM_LOCAL_EXPERTS\nE_GLOBAL = NUM_EXPERTS\n\n\ndef create_test_tensors(\n    seq_len: int = 4,  # Variable: number of tokens\n    device: str = \"cuda\",\n) -> Tuple[torch.Tensor, ...]:\n    \"\"\"\n    Create test tensors matching DeepSeek-V3 benchmark spec.\n    \n    Fixed dimensions:\n    - num_experts = 256\n    - num_local_experts = 32  \n    - hidden_size = 7168\n    - intermediate_size = 2048\n    - gemm1_out_size = 4096\n    - num_hidden_blocks = 56\n    - num_intermediate_blocks = 16\n    - num_gemm1_out_blocks = 32\n    \n    Variable:\n    - seq_len (T)\n    \"\"\"\n    T = seq_len\n    print(f\"Creating test tensors: seq_len={T}\")\n    print(f\"  Fixed: num_experts={NUM_EXPERTS}, num_local_experts={NUM_LOCAL_EXPERTS}\")\n    print(f\"         hidden_size={HIDDEN_SIZE}, intermediate_size={INTERMEDIATE_SIZE}\")\n    print(f\"         gemm1_out_size={GEMM1_OUT_SIZE}\")\n    \n    # ========================================================================\n    # Inputs (matching benchmark spec exactly)\n    # ========================================================================\n    \n    # routing_logits: float32 [seq_len, num_experts]\n    routing_logits = torch.randn(T, NUM_EXPERTS, dtype=torch.float32, device=device)\n    \n    # routing_bias: bfloat16 [num_experts]\n    routing_bias = torch.randn(NUM_EXPERTS, dtype=torch.bfloat16, device=device) * 0.1\n    \n    # hidden_states: float8_e4m3fn [seq_len, hidden_size]\n    hidden_states = torch.randn(T, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.5\n    hidden_states = hidden_states.to(torch.float8_e4m3fn)\n    \n    # hidden_states_scale: float32 [num_hidden_blocks, seq_len]\n    hidden_states_scale = torch.ones(NUM_HIDDEN_BLOCKS, T, dtype=torch.float32, device=device) * 0.5\n    \n    # gemm1_weights: float8_e4m3fn [num_local_experts, gemm1_out_size, hidden_size]\n    gemm1_weights = torch.randn(NUM_LOCAL_EXPERTS, GEMM1_OUT_SIZE, HIDDEN_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm1_weights = gemm1_weights.to(torch.float8_e4m3fn)\n    \n    # gemm1_weights_scale: float32 [num_local_experts, num_gemm1_out_blocks, num_hidden_blocks]\n    gemm1_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_GEMM1_OUT_BLOCKS, NUM_HIDDEN_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.3\n    \n    # gemm2_weights: float8_e4m3fn [num_local_experts, hidden_size, intermediate_size]\n    gemm2_weights = torch.randn(NUM_LOCAL_EXPERTS, HIDDEN_SIZE, INTERMEDIATE_SIZE, dtype=torch.float32, device=device) * 0.1\n    gemm2_weights = gemm2_weights.to(torch.float8_e4m3fn)\n    \n    # gemm2_weights_scale: float32 [num_local_experts, num_hidden_blocks, num_intermediate_blocks]\n    gemm2_weights_scale = torch.ones(\n        NUM_LOCAL_EXPERTS, NUM_HIDDEN_BLOCKS, NUM_INTERMEDIATE_BLOCKS, dtype=torch.float32, device=device\n    ) * 0.2\n    \n    # local_expert_offset: int32 scalar\n    local_expert_offset = 0\n    \n    # routed_scaling_factor: float32 scalar\n    routed_scaling_factor = 2.5\n    \n    # Pre-allocate output: bfloat16 [seq_len, hidden_size]\n    output = torch.zeros(T, HIDDEN_SIZE, dtype=torch.bfloat16, device=device)\n    \n    print(f\"Tensors created. GPU Memory: ~{torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    \n    return (\n        routing_logits,\n        routing_bias,\n        hidden_states,\n        hidden_states_scale,\n        gemm1_weights,\n        gemm1_weights_scale,\n        gemm2_weights,\n        gemm2_weights_scale,\n        local_expert_offset,\n        routed_scaling_factor,\n        output,\n    )\n\n\ndef run_torch_reference(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Pure PyTorch reference implementation for correctness checking.\n    This is a copy of the torch/kernel.py implementation.\n    \"\"\"\n    from solution.torch.kernel import run as torch_run\n    \n    # Clone output to not modify the original\n    output_ref = output.clone()\n    torch_run(\n        routing_logits,\n        routing_bias,\n        hidden_states,\n        hidden_states_scale,\n        gemm1_weights,\n        gemm1_weights_scale,\n        gemm2_weights,\n        gemm2_weights_scale,\n        local_expert_offset,\n        routed_scaling_factor,\n        output_ref,\n    )\n    return output_ref\n\n\ndef run_cutile(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n    output: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"Run CuTile implementation.\"\"\"\n    from kernel import run as cutile_run\n    \n    output_cutile = output.clone()\n    cutile_run(\n        routing_logits,\n        routing_bias,\n        hidden_states,\n        hidden_states_scale,\n        gemm1_weights,\n        gemm1_weights_scale,\n        gemm2_weights,\n        gemm2_weights_scale,\n        local_expert_offset,\n        routed_scaling_factor,\n        output_cutile,\n    )\n    return output_cutile\n\n\ndef check_correctness(\n    output_ref: torch.Tensor,\n    output_test: torch.Tensor,\n    name: str = \"CuTile\",\n    rtol: float = 1e-2,\n    atol: float = 1e-2,\n) -> bool:\n    \"\"\"Check if two outputs are close enough.\"\"\"\n    \n    # Convert to float32 for comparison\n    ref_f32 = output_ref.to(torch.float32)\n    test_f32 = output_test.to(torch.float32)\n    \n    # Compute metrics\n    abs_diff = torch.abs(ref_f32 - test_f32)\n    rel_diff = abs_diff / (torch.abs(ref_f32) + 1e-8)\n    \n    max_abs_diff = abs_diff.max().item()\n    max_rel_diff = rel_diff.max().item()\n    mean_abs_diff = abs_diff.mean().item()\n    \n    # Check if close\n    is_close = torch.allclose(ref_f32, test_f32, rtol=rtol, atol=atol)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Correctness Check: {name}\")\n    print(f\"{'='*60}\")\n    print(f\"Max absolute diff:  {max_abs_diff:.6e}\")\n    print(f\"Mean absolute diff: {mean_abs_diff:.6e}\")\n    print(f\"Max relative diff:  {max_rel_diff:.6e}\")\n    print(f\"Tolerance: rtol={rtol}, atol={atol}\")\n    print(f\"Result: {'✓ PASSED' if is_close else '✗ FAILED'}\")\n    print(f\"{'='*60}\\n\")\n    \n    return is_close\n\n\ndef benchmark(\n    func,\n    args,\n    name: str,\n    warmup: int = 10,\n    iterations: int = 50,\n) -> float:\n    \"\"\"Benchmark a function and return average time in ms.\"\"\"\n    \n    # Warmup\n    for _ in range(warmup):\n        func(*args)\n    \n    torch.cuda.synchronize()\n    \n    # Benchmark\n    start = time.perf_counter()\n    for _ in range(iterations):\n        func(*args)\n    torch.cuda.synchronize()\n    end = time.perf_counter()\n    \n    avg_ms = (end - start) / iterations * 1000\n    return avg_ms\n\n\ndef run_benchmark(seq_len: int = 16, iterations: int = 50, usesmall: bool = False):\n    \"\"\"Run benchmark comparing CuTile vs PyTorch.\"\"\"\n    \n    kernel_name = \"smallkernel.py\" if usesmall else \"kernel.py\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Benchmark: CuTile vs PyTorch MoE Kernel ({kernel_name})\")\n    print(f\"seq_len={seq_len} (variable)\")\n    print(f\"Fixed: num_experts={NUM_EXPERTS}, num_local_experts={NUM_LOCAL_EXPERTS}\")\n    print(f\"       hidden_size={HIDDEN_SIZE}, intermediate_size={INTERMEDIATE_SIZE}\")\n    print(f\"Iterations={iterations}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Create test data\n    args = create_test_tensors(seq_len=seq_len)\n    \n    # Use importlib to explicitly load from correct files\n    import importlib.util\n    import os\n    \n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    repo_root = os.path.dirname(os.path.dirname(script_dir))\n    \n    if usesmall:\n        cutile_kernel_path = os.path.join(repo_root, 'solution', 'cutile', 'smallkernel.py')\n    else:\n        cutile_kernel_path = os.path.join(repo_root, 'solution', 'cutile', 'hardcoded.py')\n    torch_kernel_path = os.path.join(repo_root, 'solution', 'torch', 'kernel.py')\n    \n    def load_kernel(kernel_path, module_name):\n        spec = importlib.util.spec_from_file_location(module_name, kernel_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n    \n    # Load CuTile kernel\n    cutile_kernel = load_kernel(cutile_kernel_path, \"cutile_kernel_bench\")\n    cutile_run = cutile_kernel.run\n    \n    # Try to load torch reference\n    has_torch_ref = False\n    torch_run = None\n    try:\n        if os.path.exists(torch_kernel_path):\n            torch_kernel = load_kernel(torch_kernel_path, \"torch_kernel_bench\")\n            torch_run = torch_kernel.run\n            has_torch_ref = True\n    except Exception as e:\n        print(f\"Warning: Could not import torch reference: {e}\")\n    \n    # Benchmark CuTile\n    cutile_time = benchmark(\n        lambda *a: cutile_run(*a),\n        args,\n        \"CuTile\",\n        iterations=iterations,\n    )\n    print(f\"CuTile:  {cutile_time:.3f} ms\")\n    \n    # Benchmark PyTorch reference\n    if has_torch_ref:\n        # Need to clone output for each run\n        def torch_run_clone(*a):\n            args_list = list(a)\n            args_list[-1] = args_list[-1].clone()  # Clone output\n            torch_run(*args_list)\n        \n        torch_time = benchmark(\n            torch_run_clone,\n            args,\n            \"PyTorch\",\n            iterations=iterations,\n        )\n        print(f\"PyTorch: {torch_time:.3f} ms\")\n        print(f\"Speedup: {torch_time / cutile_time:.2f}x\")\n    \n    print()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Test CuTile MoE kernel\")\n    parser.add_argument(\"--benchmark\", action=\"store_true\", help=\"Run benchmark\")\n    parser.add_argument(\"--seq-len\", type=int, default=16, help=\"Sequence length (number of tokens)\")\n    parser.add_argument(\"--tokens\", type=int, default=None, help=\"Alias for --seq-len\")\n    parser.add_argument(\"--iterations\", type=int, default=50, help=\"Benchmark iterations\")\n    parser.add_argument(\"--skip-correctness\", action=\"store_true\", help=\"Skip correctness check\")\n    parser.add_argument(\"--vary-seq-len\", action=\"store_true\", help=\"Test multiple seq_len values\")\n    parser.add_argument(\"--usesmall\", action=\"store_true\", help=\"Use smallkernel.py optimized for small sequences\")\n    args = parser.parse_args()\n    \n    # Handle --tokens as alias for --seq-len\n    seq_len = args.tokens if args.tokens is not None else args.seq_len\n\n    if not torch.cuda.is_available():\n        print(\"CUDA not available!\")\n        return\n    \n    print(f\"Device: {torch.cuda.get_device_name()}\")\n    print(f\"CUDA: {torch.version.cuda}\")\n    if args.usesmall:\n        print(\"Using: smallkernel.py (optimized for small sequences)\")\n    else:\n        print(\"Using: kernel.py\")\n    \n    # Setup paths - works whether run from repo root or solution/cutile/\n    import sys\n    import os\n    \n    # Get the directory containing this script\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    \n    # Determine repo root (script is at solution/cutile/test_cutile.py)\n    # So repo root is 2 levels up\n    repo_root = os.path.dirname(os.path.dirname(script_dir))\n    \n    # Define solution directories\n    cutile_dir = os.path.join(repo_root, 'solution', 'cutile')\n    torch_dir = os.path.join(repo_root, 'solution', 'torch')\n    \n    # Use importlib to explicitly load from correct files (avoid sys.path confusion)\n    import importlib.util\n    \n    def load_kernel(kernel_path, module_name):\n        \"\"\"Load a kernel module from a specific file path.\"\"\"\n        spec = importlib.util.spec_from_file_location(module_name, kernel_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n    \n    # Choose kernel file based on --usesmall flag\n    if args.usesmall:\n        cutile_kernel_path = os.path.join(cutile_dir, 'smallkernel.py')\n    else:\n        cutile_kernel_path = os.path.join(cutile_dir, 'kernel.py')\n    torch_kernel_path = os.path.join(torch_dir, 'kernel.py')\n    \n    # Run correctness check\n    if not args.skip_correctness:\n        print(\"\\n\" + \"=\"*70)\n        print(\"Running Correctness Check\")\n        print(\"=\"*70)\n        \n        test_args = create_test_tensors(seq_len=seq_len)\n        \n        # Run CuTile - explicitly load from cutile/kernel.py\n        cutile_kernel = load_kernel(cutile_kernel_path, \"cutile_kernel\")\n        output_cutile = test_args[-1].clone()\n        cutile_kernel.run(*test_args[:-1], output_cutile)\n        \n        # Run PyTorch reference\n        try:\n            if os.path.exists(torch_kernel_path):\n                torch_kernel = load_kernel(torch_kernel_path, \"torch_kernel\")\n                \n                output_torch = test_args[-1].clone()\n                torch_kernel.run(*test_args[:-1], output_torch)\n                \n                check_correctness(output_torch, output_cutile, \"CuTile vs PyTorch\")\n            else:\n                print(f\"Warning: PyTorch reference not found at {torch_kernel_path}\")\n                print(\"Skipping correctness check against reference.\")\n        except Exception as e:\n            print(f\"Warning: Could not run PyTorch reference: {e}\")\n    \n    # Run benchmark\n    if args.benchmark:\n        if args.vary_seq_len:\n            # Test multiple seq_len values\n            for sl in [1, 4, 16, 64, 256, 1024, 2048, 4096, 8192, 16384]:\n                try:\n                    run_benchmark(seq_len=sl, iterations=args.iterations, usesmall=args.usesmall)\n                except torch.cuda.OutOfMemoryError:\n                    print(f\"OOM at seq_len={sl}, stopping\")\n                    break\n        else:\n            run_benchmark(seq_len=seq_len, iterations=args.iterations, usesmall=args.usesmall)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    }
  ],
  "description": ""
}